{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information gain ratio tensor(0.5319)\n",
      "Information gain ratio tensor(0.7621)\n"
     ]
    }
   ],
   "source": [
    "# PROTOTYPING \n",
    "\n",
    "\n",
    "\n",
    "num_diffusion_timesteps=1000\n",
    "scale = 1000 / num_diffusion_timesteps\n",
    "beta_start = scale * 1e-4\n",
    "beta_end = scale * 0.02\n",
    "beta = torch.linspace(\n",
    "    beta_start,\n",
    "    beta_end,\n",
    "    num_diffusion_timesteps,\n",
    ")\n",
    "\n",
    "beta = torch.clamp(beta, 0, 0.999)\n",
    "\n",
    "alpha = 1 - beta\n",
    "self_sqrt_beta                       = torch.sqrt(beta)\n",
    "alpha_cumulative                = torch.cumprod(alpha, dim=0)\n",
    "#alpha_cumulative = torch.clamp(alpha_cumulative, 0, 0.999)\n",
    "sqrt_alpha_cumulative           = torch.sqrt(alpha_cumulative)\n",
    "one_by_sqrt_alpha               = 1. / torch.sqrt(alpha)\n",
    "sqrt_one_minus_alpha_cumulative = torch.sqrt(1 - alpha_cumulative)\n",
    "\n",
    "x0s = torch.zeros(5)\n",
    "x0s[0] = 1\n",
    "\n",
    "\n",
    "\n",
    "def forward_diffusion(x0, timesteps):\n",
    "    eps = torch.rand_like(x0)  # Noise\n",
    "    mean    = sqrt_alpha_cumulative[timesteps] * x0  # Image scaled\n",
    "\n",
    "    std_dev = sqrt_one_minus_alpha_cumulative[timesteps] # Noise scaled\n",
    "    sample  = mean + std_dev * eps # scaled inputs * scaled noise\n",
    "\n",
    "    return sample, eps\n",
    "\n",
    "\n",
    "\n",
    "noisy_images = []\n",
    "specific_timesteps = [0, 10, 50, 100, 150, 200, 250, 300, 400, 600, 800, 999]\n",
    " \n",
    "\n",
    "\n",
    "def sf(x):\n",
    "    return torch.softmax(x,0)\n",
    "def entropy(x):\n",
    "    return -torch.sum((x) * torch.log2((x)))\n",
    "\n",
    "def split_info(k_tot, k_i):\n",
    "    s1 = torch.sum(k_tot)\n",
    "    s2 = torch.sum(k_i)\n",
    "    r1 = s2/s1\n",
    "    r1_res = 1-r1\n",
    "    return torch.sum(entropy(r1) + entropy(r1_res))\n",
    "   \n",
    "def IG(x0, x1):\n",
    "    return entropy(sf(x0)) - entropy(sf(x1))\n",
    "\n",
    "def IGR(x0, x1, k0, k1):\n",
    "    return IG(x0, x1) / split_info(k0, k1)\n",
    "\n",
    "\n",
    "\n",
    "x1 = torch.rand(5)\n",
    "x0 = torch.ones(7) * -0.1\n",
    "x0[0:4] = x1[:-1]\n",
    "\n",
    "k1 = torch.ones(5)\n",
    "k0 = torch.ones(7)\n",
    "\n",
    "print('Information gain ratio', IGR(x0, x1, k0, k1))\n",
    "\n",
    "x0 = torch.ones(7) * -0.1\n",
    "x0[0:5] = x1\n",
    "print('Information gain ratio', IGR(x0, x1, k0, k1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU NN\n",
    "\n",
    "class GRU_WFC(nn.Module):\n",
    "    def __init__(self,tile_num, num_match, H, W, iter, kernel_size = 3, bias_scale = 0.25, deep = False):\n",
    "        super().__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        RAFT has a few more NNs in between the GRU update block. This is probably important cause im pretty sure this netwrok is too small to handle this problem\n",
    "        \"\"\"\n",
    "        self.num_tile = tile_num\n",
    "        self.map_size = H*W\n",
    "        self.W = W\n",
    "        self.H = H\n",
    "        self.deep = deep\n",
    "        self.num_iter = iter\n",
    "        self.bias_scale = bias_scale\n",
    "        self.num_matches = num_match\n",
    "        \n",
    "        self.valid = torch.zeros((self.num_matches, H, W)) / 4.0\n",
    "        self.flat_valid = torch.zeros((self.num_matches, H * W))\n",
    "        self.mask = torch.ones((tile_num, H, W))\n",
    "        self.vmask = torch.zeros((4,H, W))\n",
    "\n",
    "        if 0:\n",
    "            self.conv1 = torch.nn.Conv2d(self.num_tile, self.num_tile, 5,1,2) # We append binary validity vector to each space of size = number of possible connections\n",
    "            self.conv2 = torch.nn.Conv2d(self.num_tile, self.num_tile, 5,1,2) \n",
    "            self.conv3 = torch.nn.Conv2d(self.num_tile,self.num_tile, 5,1,2)\n",
    "\n",
    "        self.k_size = kernel_size\n",
    "        self.p_size = kernel_size//2\n",
    "\n",
    "        # Separable convolutions taken straight from RAFT. Just rewrite at some point, or change for another faster convolution idk\n",
    "        self.convz1 = nn.Conv2d(self.num_tile, self.num_tile, (1,self.k_size), padding=(0,self.p_size))\n",
    "        self.convr1 = nn.Conv2d(self.num_tile, self.num_tile, (1,self.k_size), padding=(0,self.p_size))\n",
    "        self.convq1 = nn.Conv2d(self.num_tile*2, self.num_tile, (1,self.k_size), padding=(0,self.p_size))\n",
    "\n",
    "        self.convz2 = nn.Conv2d(self.num_tile, self.num_tile, (self.k_size,1), padding=(self.p_size,0))\n",
    "        self.convr2 = nn.Conv2d(self.num_tile, self.num_tile, (self.k_size,1), padding=(self.p_size,0))\n",
    "        self.convq2 = nn.Conv2d(self.num_tile*2, self.num_tile, (self.k_size,1), padding=(self.p_size,0))\n",
    "\n",
    "        self.inter_cv1 = nn.Conv2d(self.num_tile, self.num_tile*2, (1,self.k_size), padding=(0,self.p_size))\n",
    "        self.inter_cv2 = nn.Conv2d(self.num_tile, self.num_tile//2, (1,self.k_size), padding=(0,self.p_size))\n",
    "        self.inter_cv3 = nn.Conv2d(self.num_tile*2, self.num_tile, (1,self.k_size), padding=(0,self.p_size))\n",
    "        self.inter_cv4 = nn.Conv2d(self.num_tile//2, self.num_tile, (1,self.k_size), padding=(0,self.p_size))\n",
    "\n",
    "        self.inter_ch1 = nn.Conv2d(self.num_tile, self.num_tile*2, (self.k_size,1), padding=(self.p_size,0))\n",
    "        self.inter_ch2 = nn.Conv2d(self.num_tile, self.num_tile//2, (self.k_size,1), padding=(self.p_size,0))\n",
    "        self.inter_ch3 = nn.Conv2d(self.num_tile*2, self.num_tile, (self.k_size,1), padding=(self.p_size,0))\n",
    "        self.inter_ch4 = nn.Conv2d(self.num_tile//2, self.num_tile, (self.k_size,1), padding=(self.p_size,0))\n",
    "\n",
    "        self.inter_skip = nn.Conv2d(self.num_tile*2, self.num_tile, self.k_size, 1, self.p_size)\n",
    "        self.inter_skip_gate = nn.Conv2d(self.num_tile, self.num_tile, self.k_size, 1, self.p_size)\n",
    "\n",
    "        self.sigma = torch.nn.Sigmoid()\n",
    "        self.phi = torch.nn.Tanh()\n",
    "        self.lRel = torch.nn.LeakyReLU(0.2)\n",
    "        self.pad = torch.nn.ZeroPad2d(1)\n",
    "       \n",
    "\n",
    "    def forward(self, h, steps, training):\n",
    "        \"\"\"\n",
    "        input: initial white noise block shape [manifold number, tile count, height, width], training boolean: if true retain each diffusion prediciton, if false only retain final map\n",
    "        returns: either collapsed map or collapsed map with intermediary diffusion steps for training \n",
    "        \"\"\"\n",
    "        collapsed = False\n",
    "        iter = 0\n",
    "\n",
    "\n",
    "        while (iter < steps-1):\n",
    "            #h = self.collapse(h, k)\n",
    "            # Check for collapse or something\n",
    "            # h = [1, num_tiles, w, h]\n",
    "            if self.deep:\n",
    "                h = self.deeper_collapse(h)\n",
    "            if 0:\n",
    "\n",
    "                xskipv = h.clone()\n",
    "                xskiph = h.clone()\n",
    "                \n",
    "                v1 = self.inter_cv1(xskipv)\n",
    "                v2 = self.phi(self.inter_cv3(v1))\n",
    "                v3 = self.lRel(self.inter_cv2(xskipv))\n",
    "                v4 = self.sigma(self.inter_cv4(v3))\n",
    "                xskipv = xskipv * (1 - v4) + v2 * self.bias_scale\n",
    "    \n",
    "                h1 = self.inter_ch1(xskiph)\n",
    "                h2 = self.phi(self.inter_ch3(h1))\n",
    "                h3 = self.lRel(self.inter_ch2(xskiph))\n",
    "                h4 = self.sigma(self.inter_ch4(h3))\n",
    "                xskiph = xskiph * (1 - h4) + h2 * self.bias_scale\n",
    "\n",
    "                xskip = torch.concat((xskipv, xskiph), dim=1)\n",
    "                xskip = self.sigma(self.inter_skip(xskip))\n",
    "\n",
    "                gate = self.sigma(self.inter_skip_gate(h))\n",
    "                h = h * (1 - gate) + self.phi(xskip) * gate # Scale reduction for stability hopefully\n",
    "\n",
    "                h = h * self.mask\n",
    "\n",
    "            h = self.collapse(h)\n",
    "\n",
    "            h = h #* self.mask\n",
    "            \n",
    "            if training:\n",
    "                if iter == 0:\n",
    "                    h_ret = h.clone().unsqueeze(1)\n",
    "                else:\n",
    "                    h_ret = torch.concat((h_ret, h.clone().unsqueeze(1)), dim = 1)\n",
    "\n",
    "            if collapsed:\n",
    "                break\n",
    "            iter += 1\n",
    "            \n",
    "        if training:\n",
    "            return h_ret\n",
    "        else:\n",
    "            return h\n",
    "        \n",
    "    def collapse(self, h): # h is memory block. becomes map after pseudo wfc, key matrix\n",
    "            \n",
    "            #self.key_query(h, k)\n",
    "            #print('-----',h.shape, self.valid.shape)\n",
    "            x = h #torch.concat((h,self.valid.unsqueeze(0)),dim=1) # Append a binary vector to each wave space representing valididty w.r.t. each neighbor\n",
    "\n",
    "            # horizontal\n",
    "            hx = x\n",
    "            z = torch.sigmoid(self.convz1(hx))\n",
    "            r = torch.sigmoid(self.convr1(hx))\n",
    "            q = torch.tanh(self.convq1(torch.concat((r*h, x),dim=1)))        \n",
    "            h = (1-z) * h + z * q\n",
    "\n",
    "            # vertical\n",
    "            hx = x\n",
    "            z = torch.sigmoid(self.convz2(hx))\n",
    "            r = torch.sigmoid(self.convr2(hx))\n",
    "            q = torch.tanh(self.convq2(torch.concat((r*h, x),dim=1)))       \n",
    "            h = (1-z) * h + z * q\n",
    "\n",
    "            return h\n",
    "    \n",
    "    def deeper_collapse(self, h):\n",
    "        xskipv = h.clone()\n",
    "        xskiph = h.clone()\n",
    "        \n",
    "        v1 = self.inter_cv1(xskipv)\n",
    "        v2 = self.phi(self.inter_cv3(v1))\n",
    "        v3 = self.lRel(self.inter_cv2(xskipv))\n",
    "        v4 = self.sigma(self.inter_cv4(v3))\n",
    "        xskipv = xskipv * (1 - v4) + v2 * self.bias_scale\n",
    "\n",
    "        h1 = self.inter_ch1(xskiph)\n",
    "        h2 = self.phi(self.inter_ch3(h1))\n",
    "        h3 = self.lRel(self.inter_ch2(xskiph))\n",
    "        h4 = self.sigma(self.inter_ch4(h3))\n",
    "        xskiph = xskiph * (1 - h4) + h2 * self.bias_scale\n",
    "\n",
    "        xskip = torch.concat((xskipv, xskiph), dim=1)\n",
    "        xskip = self.sigma(self.inter_skip(xskip))\n",
    "\n",
    "        gate = self.sigma(self.inter_skip_gate(h))\n",
    "        h = h * (1 - gate) + self.phi(xskip) * gate # Scale reduction for stability hopefully\n",
    "\n",
    "        h = h * self.mask\n",
    "\n",
    "        return h\n",
    "\n",
    "    def key_query(self, x, key):\n",
    "        x = (torch.softmax(x.clone(), dim=1))\n",
    "        x_pad = x.clone()\n",
    "   \n",
    "        y_right = x_pad[0, :, :, :-1].flatten(-2,-1)\n",
    "        y_left = x_pad[0, :, :, 1:].flatten(-2,-1)\n",
    "        y_up = x_pad[0, :, :-1, :].flatten(-2,-1)\n",
    "        y_down = x_pad[0, :, 1:, :].flatten(-2,-1)\n",
    "\n",
    "\n",
    "        #--------- Conditional joint probbility [ C =  (1 - sum([Y x X] * K) / X) ] + inf/nan mask\n",
    "        if 1:\n",
    "            pu_wrt_d = self.pad(self.conprob(y_up, y_down, key).permute(2,1,0).reshape(2, self.H-1, self.W))\n",
    "            pd_wrt_u = self.pad(self.conprob(y_down, y_up, key).permute(2,1,0).reshape(2, self.H-1, self.W))\n",
    "            pl_wrt_r = self.pad(self.conprob(y_right, y_left, key).permute(2,1,0).reshape(2,self.H,self.W-1))\n",
    "            pr_wrt_l = self.pad(self.conprob(y_left, y_right, key).permute(2,1,0).reshape(2,self.H,self.W-1))\n",
    "\n",
    "        #--------- Oringial = [ C = Y x K x X ]\n",
    "        if 0:\n",
    "            pred_v = torch.matmul(key, y_up.squeeze())\n",
    "            #pred_v = pred_v.T\n",
    "            error_v =  y_down * pred_v # torch.matmul(y_down.permute(1,0).unsqueeze(1), pred_v.unsqueeze(-1)).reshape(1, self.H-1, self.W)\n",
    "\n",
    "            pred_vd = torch.matmul(key, y_down.squeeze())\n",
    "            error_vd = y_up * pred_vd\n",
    "            #pred_vd = pred_vd.T\n",
    "            #error_vd = torch.matmul(y_up.permute(1,0).unsqueeze(1), pred_vd.unsqueeze(-1)).reshape(1, self.H-1, self.W)\n",
    "\n",
    "            pred_h = torch.matmul(key, y_right.squeeze())\n",
    "            error_h = y_left * pred_h\n",
    "            #pred_h = pred_h.T\n",
    "            #error_h = torch.matmul(y_left.permute(1,0).unsqueeze(1), pred_h.unsqueeze(-1)).reshape(1,self.H,self.W-1)\n",
    "\n",
    "            pred_hl = torch.matmul(key, y_left.squeeze())\n",
    "            error_hl = (y_right * pred_hl)\n",
    "            #pred_hl = pred_hl.T\n",
    "            #error_hl = torch.matmul(y_right.permute(1,0).unsqueeze(1), pred_hl.unsqueeze(-1)).reshape(1,self.H,self.W-1)\n",
    "\n",
    "            error_v = self.pad(error_v.reshape(2, self.H-1, self.W))\n",
    "            error_h = self.pad(error_h.reshape(2,self.H,self.W-1))\n",
    "            error_vd = self.pad(error_vd.reshape(2, self.H-1, self.W))\n",
    "            error_hl = self.pad(error_hl.reshape(2,self.H,self.W-1))\n",
    "\n",
    "        \n",
    "        error_v = pu_wrt_d\n",
    "        bd = error_v[:,1:,1:-1] # \n",
    "        bd[:,9] =(bd[:,9] - 1) # -1 on bottom most row\n",
    "\n",
    "        error_vd = pd_wrt_u\n",
    "        bu = error_vd[:,:-1,1:-1]\n",
    "        bu[:,0] = (bu[:,0] - 1) # -1 on top most row\n",
    "\n",
    "        error_h = pr_wrt_l\n",
    "        bl= error_h[:,1:-1,:-1].transpose(-2,-1)\n",
    "        bl[:,0] = (bl[:,0] - 1) # -1 on left most column\n",
    "\n",
    "        error_hl = pl_wrt_r\n",
    "        br = error_hl[:,1:-1,1:].transpose(-2,-1)\n",
    "        br[:,9,:] = (br[:,9,:] - 1).squeeze() # -1 of right most column\n",
    "\n",
    "        vmask = torch.concat((bd.unsqueeze(0),bu.unsqueeze(0),bl.transpose(-2,-1).unsqueeze(0),br.transpose(-2,-1).unsqueeze(0)),dim=1)        \n",
    "        self.reset_mask()\n",
    "        ones = torch.ones(8, self.H, self.W)\n",
    "        self.valid =  ones * vmask.squeeze()\n",
    "        self.vmask = vmask.clone()\n",
    "\n",
    "    def conprob(self, x, y, k):\n",
    "        # Computes the conditional joint probability between two wave spaces\n",
    "        t1 = x.transpose(-2,-1).unsqueeze(1).clone()\n",
    "        t2 = y.transpose(-2,-1).unsqueeze(1).clone()\n",
    "        keytemp = k.clone()\n",
    "\n",
    "        test = torch.matmul(t2.transpose(-2,-1), t1)\n",
    "        test = torch.sum((keytemp.unsqueeze(0).unsqueeze(0) * test.unsqueeze(1)), dim = 2)\n",
    "        test = self.infnan_mask(test)\n",
    "\n",
    "\n",
    "        return test\n",
    "\n",
    "    def reset_mask(self):\n",
    "        self.valid = self.valid * 0\n",
    "        self.valid = self.valid + 1/8.0\n",
    "\n",
    "    def infnan_mask(self, x):\n",
    "        # ******* This function is taken straight from stack overflow not my code: https://stackoverflow.com/questions/64594493/filter-out-nan-values-from-a-pytorch-n-dimensional-tensor\n",
    "        shape = x.shape\n",
    "        tensor_reshaped = x.reshape(shape[0],-1).clone()\n",
    "        #Drop all rows containing any nan:\n",
    "        tensor_reshaped = tensor_reshaped[~torch.any(tensor_reshaped.isnan(),dim=1)]\n",
    "        # I just copied the same thing but for inf\n",
    "        tensor_reshaped = tensor_reshaped[~torch.any(tensor_reshaped.isinf(),dim=1)]\n",
    "        #Reshape back:\n",
    "        x = tensor_reshaped.reshape(tensor_reshaped.shape[0],*shape[1:])\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def wfc_loss(map, key):\n",
    "    \"\"\"\n",
    "    Takes predicted map and returns a bainary map indicating valid vetical and horizontal connections accoriding the the maximum probability\n",
    "    \"\"\"\n",
    "    x = torch.softmax(map.squeeze().flatten(-2,-1), dim=0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        val, id = x.max(dim=0, keepdim = True)\n",
    "        mask = torch.zeros_like(x)\n",
    "        id_l = torch.arange(100)\n",
    "        mask[id[0,:],id_l[:]] = 1\n",
    "\n",
    "    x_pad = x.reshape(map.shape) #torch.ceil((x * mask)).reshape(map.shape)\n",
    "   \n",
    "    y_right = x_pad[0, :, :, :-1].flatten(-2,-1)\n",
    "    y_left = x_pad[0, :, :, 1:].flatten(-2,-1)\n",
    "    y_up = x_pad[0, :, :-1, :].flatten(-2,-1)\n",
    "    y_down = x_pad[0, :, 1:, :].flatten(-2,-1)\n",
    "\n",
    "    pred_v = torch.matmul(key, y_up.squeeze())\n",
    "    pred_v = pred_v.T\n",
    "    error_v = torch.matmul(y_down.permute(1,0).unsqueeze(1), pred_v.unsqueeze(-1))\n",
    "\n",
    "    pred_vd = torch.matmul(key, y_down.squeeze())\n",
    "    pred_vd = pred_vd.T\n",
    "    error_vd = torch.matmul(y_up.permute(1,0).unsqueeze(1), pred_vd.unsqueeze(-1))\n",
    "\n",
    "    pred_h = torch.matmul(key, y_right.squeeze())\n",
    "    pred_h = pred_h.T\n",
    "    error_h = torch.matmul(y_left.permute(1,0).unsqueeze(1), pred_h.unsqueeze(-1))\n",
    "\n",
    "    pred_hl = torch.matmul(key, y_left.squeeze())\n",
    "    pred_hl = pred_hl.T\n",
    "    error_hl = torch.matmul(y_right.permute(1,0).unsqueeze(1), pred_hl.unsqueeze(-1))\n",
    "\n",
    "    return error_v, error_vd, error_h, error_hl\n",
    "\n",
    "def infnan_mask(x):\n",
    "        # ******* This function is taken straight from stack overflow not my code: https://stackoverflow.com/questions/64594493/filter-out-nan-values-from-a-pytorch-n-dimensional-tensor\n",
    "        shape = x.shape\n",
    "        tensor_reshaped = x.reshape(shape[0],-1).clone()\n",
    "        #Drop all rows containing any nan:\n",
    "        tensor_reshaped = tensor_reshaped[~torch.any(tensor_reshaped.isnan(),dim=1)]\n",
    "        # I just copied the same thing but for inf\n",
    "        tensor_reshaped = tensor_reshaped[~torch.any(tensor_reshaped.isinf(),dim=1)]\n",
    "        #Reshape back:\n",
    "        x = tensor_reshaped.reshape(tensor_reshaped.shape[0],*shape[1:])\n",
    "\n",
    "        return x\n",
    "\n",
    "def conprob(x, y, k):\n",
    "        # Computes the conditional joint probability between two wave spaces\n",
    "        t1 = x.transpose(-2,-1).unsqueeze(1).clone()\n",
    "        t2 = y.transpose(-2,-1).unsqueeze(1).clone()\n",
    "        keytemp = k.clone()\n",
    "        \n",
    "        test = torch.matmul(t2.transpose(-2,-1), t1)\n",
    "        test = torch.sum((keytemp.unsqueeze(0).unsqueeze(0) * test.unsqueeze(1)), dim = 2)\n",
    "        test = infnan_mask(test)\n",
    "\n",
    "        return test\n",
    "\n",
    "smax = torch.nn.Softmax(dim=1)\n",
    "def wfc_loss_2(map, key):\n",
    "    map = (torch.softmax(map, dim=1))\n",
    "    x_pad = map\n",
    "\n",
    "    y_right = x_pad[0, :, :, :-1].flatten(-2,-1)\n",
    "    y_left = x_pad[0, :, :, 1:].flatten(-2,-1)\n",
    "    y_up = x_pad[0, :, :-1, :].flatten(-2,-1)\n",
    "    y_down = x_pad[0, :, 1:, :].flatten(-2,-1)\n",
    "\n",
    "    #--------- Conditional joint probbility [ C =  (1 - sum([Y x X] * K) / X) ] + inf/nan mask\n",
    "    \n",
    "    p1 = (conprob(y_up, y_down, key).permute(2,1,0))\n",
    "    p2 = (conprob(y_down, y_up, key).permute(2,1,0))\n",
    "    p3 = (conprob(y_right, y_left, key).permute(2,1,0))\n",
    "    p4= (conprob(y_left, y_right, key).permute(2,1,0))\n",
    "\n",
    "    return p1, p2, p3, p4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACTUAL WFC\n",
    "\n",
    "def actual_wfc(tile_space, width, height, key, seed, z):\n",
    "\n",
    "    wave_map = torch.ones((tile_space, width, height))\n",
    "    collapse_index = torch.zeros(width * height)\n",
    "\n",
    "    #if seed != None:\n",
    "    wave_map = wave_map * seed\n",
    "    collapse_index[z] = 1\n",
    "\n",
    "    wave_map = diffuse(wave_map, z//width, z%width, key, width, height)\n",
    "\n",
    "    key = key\n",
    "    collapsed = False\n",
    "    count = 0\n",
    "    if 1:\n",
    "        while not collapsed:\n",
    "            id_lin = choose(wave_map, collapse_index)\n",
    "            if id_lin >= 0:\n",
    "                cx = id_lin % width\n",
    "                cy = id_lin // width\n",
    "\n",
    "                wave_vec = wave_map[:,cy, cx].squeeze() #key_return(wave_map, cy, cx, height, width, key, 4)\n",
    "                tile = collapse(wave_vec)\n",
    "\n",
    "                wave_map[:, cy, cx] = tile\n",
    "                wave_map = diffuse(wave_map, cy, cx, key, width, height)\n",
    "                \n",
    "                collapse_index[cx + cy * width] = 1\n",
    "\n",
    "                if collapse_index.sum() == width * height or count > width*height + 10:\n",
    "                    collapsed = True\n",
    "                    if count > width*height + 10:\n",
    "                        print('out of time')\n",
    "                    break\n",
    "\n",
    "                count += 1\n",
    "\n",
    "    return wave_map\n",
    "\n",
    "def diffuse(map_, cy, cx, k, w, h):\n",
    "    cy = int(cy)\n",
    "    cx = int(cx)\n",
    "    vec = map_[:, cy, cx].unsqueeze(-1)\n",
    "\n",
    "    if cy-1 >= 0:\n",
    "        map_[:, cy-1,cx] = map_[:, cy-1,cx] * (torch.matmul(k, vec).squeeze())#.unsqueeze(-1)\n",
    "    if cy+1 < h:\n",
    "        map_[:, cy+1,cx] = map_[:, cy+1,cx] * (torch.matmul(k, vec).squeeze())#.unsqueeze(-1)\n",
    "    if cx-1 >= 0:\n",
    "        map_[:, cy, cx-1] = map_[:, cy, cx-1] * (torch.matmul(k, vec).squeeze())#.unsqueeze(-1)\n",
    "    if cx+1 < w:\n",
    "        map_[:, cy, cx+1] = map_[:, cy, cx+1] * (torch.matmul(k, vec).squeeze())#.unsqueeze(-1)\n",
    "\n",
    "    return map_\n",
    "\n",
    "def collapse(x): \n",
    "    # X is current wavespace legality key\n",
    "    \n",
    "    probability = torch.softmax(torch.rand_like(x), dim=0) #*torch.softmax(freq, dim=0)\n",
    "    likelihood = x * probability #torch.matmul(key.T, probability.T)\n",
    "    _, tile_choice = torch.max(likelihood, dim=0)\n",
    "    x_collapse = torch.zeros_like(x)\n",
    "    x_collapse[tile_choice] = 1\n",
    "\n",
    "    return x_collapse\n",
    "    \n",
    "def key_return(map, cy, cx, h, w, key, num_adj):\n",
    "    \"\"\"\n",
    "    Unused cause it was wrong\n",
    "    \n",
    "    \"\"\"\n",
    "    vspace = torch.zeros(4, map.shape[0])\n",
    "    epsilon = 1e-3\n",
    "    if cy-1 >= 0:\n",
    "        vspace[0] = map[:,cy-1,cx]\n",
    "    else:\n",
    "        vspace[0,:] = 1\n",
    "    if cy+1 < h:\n",
    "        vspace[1] = map[:,cy+1,cx]\n",
    "    else:\n",
    "        vspace[1,:] = 1\n",
    "    if cx-1 >= 0:\n",
    "        vspace[2] = map[:,cy, cx-1]\n",
    "    else:\n",
    "        vspace[2,:] = 1\n",
    "    if cx+1 < w:\n",
    "        vspace[3] = map[:,cy, cx+1]\n",
    "    else:\n",
    "        vspace[3,:] = 1\n",
    "\n",
    "    vspace = torch.clamp(torch.matmul(key.unsqueeze(0), vspace.unsqueeze(-1)).squeeze(), 0, 1)\n",
    "    vspace = (torch.sum(vspace, dim=0) + epsilon) // num_adj\n",
    "\n",
    "    return vspace\n",
    "\n",
    "def choose(map_, collapse_list):\n",
    "    x_sum = torch.sum(map_,dim=0)\n",
    "    x_sumf = x_sum.flatten()\n",
    "    v, id = x_sumf.sort()\n",
    "    idx = -1\n",
    "\n",
    "    for k in range((v.shape[0])):\n",
    "        if collapse_list[id[k]] == 0:\n",
    "            idx = id[k]\n",
    "            break\n",
    "\n",
    "    return int(idx)\n",
    "\n",
    "def validator(x, key):\n",
    "    valid = True\n",
    "    vmap = torch.ones_like(x)\n",
    "\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            tile = int(x[j, i])\n",
    "            \n",
    "            if j-1 >= 0:\n",
    "                if key[tile, int(x[j-1,i])] == 0:\n",
    "                    valid = False\n",
    "                    vmap[j,i] -= 1\n",
    "\n",
    "            if j+1 < x.shape[1]:\n",
    "                if key[tile, int(x[j+1,i])] == 0:\n",
    "                    valid = False\n",
    "                    vmap[j,i] -= 1     \n",
    "\n",
    "            if i-1 >= 0:\n",
    "                if key[tile, int(x[j,i-1])] == 0:\n",
    "                    valid = False\n",
    "                    vmap[j,i] -= 1\n",
    "\n",
    "            if i+1 < x.shape[0]:\n",
    "                if key[tile, int(x[j,i+1])] == 0:\n",
    "                    valid = False\n",
    "                    vmap[j,i] -= 1\n",
    " \n",
    "\n",
    "    return valid, torch.clamp(vmap, 0, 1)\n",
    "\n",
    "def gen_key(num_tiles, h, w):\n",
    "    map_init = torch.randint(0, num_tiles, (h, w)) # [H, W]\n",
    "    #  Key format: KEY[VALUE, QUERY] -> query is neighbor, value is us tile, returns validity of us given neighbor\n",
    "\n",
    "    key = torch.zeros(( num_tiles, num_tiles))# [1, N, N]\n",
    "    for j in range(w):\n",
    "        for i in range(h):\n",
    "            \n",
    "            if i+1 < h:\n",
    "                key[map_init[i, j], map_init[i+1, j]] = 1 \n",
    "            if i-1 >= 0:\n",
    "                key[map_init[i, j], map_init[i-1, j]] = 1 \n",
    "\n",
    "            if j+1 < w:\n",
    "                key[map_init[i, j], map_init[i, j+1]] = 1 \n",
    "            if j-1 >= 0:\n",
    "                key[map_init[i, j], map_init[i, j-1]] = 1 \n",
    "\n",
    "    return key\n",
    "\n",
    "def gen_seed(num_tiles, height, width):\n",
    "    seed = torch.ones((num_tiles, width, height))\n",
    "    xy = torch.randint(0, height, (2,1))\n",
    "    z = torch.randint(0, num_tiles, (1,1))\n",
    "    seed[:,xy[0], xy[1]] = seed[:,xy[0], xy[1]]*0\n",
    "    seed[z,xy[0], xy[1]] = 1\n",
    "\n",
    "    return seed, xy[1]+xy[0]*width\n",
    "\n",
    "def gen_map(key, h, w, nt):\n",
    "\n",
    "    iter = 0\n",
    "    valid = False\n",
    "\n",
    "    while valid == False:\n",
    "        valid = False\n",
    "        seed, z = gen_seed(nt, h, w)\n",
    "        map_ = actual_wfc(nt, w, h, key, seed, z).squeeze()\n",
    "        val, id = torch.max(map_.flatten(-2,-1),dim=0)\n",
    "        id_map = id.reshape(h,w)\n",
    "        valid, vmap = validator(id_map, key)\n",
    "        iter += 1\n",
    "    \n",
    "    if iter > 10:\n",
    "        print('shit')\n",
    "    \n",
    "    return map_, id_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIFFUSION CLASS \n",
    "\n",
    "class diffusion():\n",
    "    def __init__(self):\n",
    "        self.num_diffusion_timesteps=1000\n",
    "        scale = 1000 / self.num_diffusion_timesteps\n",
    "        beta_start = scale * 1e-4\n",
    "        beta_end = scale * 0.02\n",
    "        beta = torch.linspace(\n",
    "            beta_start,\n",
    "            beta_end,\n",
    "            self.num_diffusion_timesteps,\n",
    "        )\n",
    "\n",
    "        beta = torch.clamp(beta, 0, 0.999)\n",
    "\n",
    "        self.alpha = 1 - beta\n",
    "        self.self_sqrt_beta = torch.sqrt(beta)\n",
    "        self.alpha_cumulative = torch.cumprod(self.alpha, dim=0)\n",
    "        #alpha_cumulative = torch.clamp(alpha_cumulative, 0, 0.999)\n",
    "        self.sqrt_alpha_cumulative = torch.sqrt(self.alpha_cumulative)\n",
    "        self.one_by_sqrt_alpha = 1. / torch.sqrt(self.alpha)\n",
    "        self.sqrt_one_minus_alpha_cumulative = torch.sqrt(1 - self.alpha_cumulative)\n",
    "\n",
    "\n",
    "    def forward_diffusion(self, x0, timesteps, beta_time, epsilon_prior = 0):\n",
    "\n",
    "        if epsilon_prior.sum() == 0: # We diffuse forward, then check for entropy threshold, then if exceeded we go back in time and idffuse new keys forwar as well with same epsilon\n",
    "            eps = torch.rand_like(x0) \n",
    "        else:\n",
    "            eps = epsilon_prior\n",
    "        \n",
    "        indices = timesteps[beta_time]\n",
    "        mean = self.sqrt_alpha_cumulative[indices] * x0  # Map scaled\n",
    "        std_dev = self.sqrt_one_minus_alpha_cumulative[indices] # Noise scaled\n",
    "        sample  = mean + std_dev * eps # scaled inputs * scaled noise\n",
    "\n",
    "        return sample, eps\n",
    "\n",
    "    def do_diffusion(self, x0, key):\n",
    "        \n",
    "        # x0 is a 3d\n",
    "\n",
    "        noisy_images = x0.squeeze().clone().unsqueeze(0)\n",
    "        xts_threshold = x0.clone()\n",
    "        xts = x0.clone()\n",
    "\n",
    "        specific_timesteps = [0, 10, 50, 100, 150, 200, 250, 300, 400, 600, 800, 999] # This less represents the \"timesteps\" in diffusion and more the \"rate\" of diffusion. \n",
    "        timestep = torch.as_tensor(specific_timesteps, dtype=torch.long)\n",
    "        beta_index = timestep.shape[0]\n",
    "        beta_time = torch.zeros_like(x0)\n",
    "        z = torch.zeros(1)\n",
    "        #beta_time = torch.as_tensor(beta_time, dtype=torch.long)\n",
    "\n",
    "        x_key_mask = self.uncollapse(x0.clone(), key)\n",
    "\n",
    "        diffusing = True\n",
    "        itr = 0\n",
    "        while diffusing:\n",
    "            xPrior = xts\n",
    "            betaPrior = beta_time.clone()\n",
    "\n",
    "            # Take forward diffusion step\n",
    "            xts, eps = self.forward_diffusion(xPrior.clone(), timestep, torch.as_tensor(beta_time, dtype=torch.long), z)\n",
    "\n",
    "            # Update mask according to entropy threshold -> uncollapse valididty \n",
    "            xts_threshold[beta_time >= 3] = 1\n",
    "            x_key_mask = uncollapse(xts_threshold, key) \n",
    "            beta_time += torch.ones_like(beta_time) * x_key_mask\n",
    "            beta_time = torch.clamp(beta_time, 0, beta_index-1)\n",
    "\n",
    "            # \"re-perform\" diffusion using retcon'd beta index\n",
    "            xts, _ = self.forward_diffusion(xPrior.clone(), timestep, torch.as_tensor(beta_time, dtype=torch.long), eps)\n",
    "\n",
    "            noisy_images = torch.concat((xts.unsqueeze(0), noisy_images), dim = 0)\n",
    "\n",
    "            itr += 1\n",
    "\n",
    "            if betaPrior.sum() == beta_time.sum():\n",
    "                # Beta time will increase each loop as we march through diffusion, but will stay static when we finish or cannot uncollapse any more\n",
    "                diffusing = False\n",
    "\n",
    "        return noisy_images, itr\n",
    "    \n",
    "    def uncollapse(self, wave_map, key):\n",
    "        diff_map = torch.ones_like(wave_map)\n",
    "\n",
    "        for i in range(wave_map.shape[1]):\n",
    "            for j in range(wave_map.shape[2]):\n",
    "                \n",
    "                if j-1 >= 0:\n",
    "                    diff_map[:,j,i] += diff_map[:,j,i]*(torch.matmul(key, wave_map[:, j-1, i]))#/wave_map[:, j-1, i].sum())\n",
    "\n",
    "                if j+1 < wave_map.shape[1]:\n",
    "                    diff_map[:,j,i] += diff_map[:,j,i]*(torch.matmul(key, wave_map[:, j+1, i]))#/wave_map[:, j+1, i].sum())\n",
    "                        \n",
    "                if i-1 >= 0:\n",
    "                    diff_map[:,j,i] += diff_map[:,j,i]*(torch.matmul(key, wave_map[:, j, i-1]))#/wave_map[:, j, i-1].sum())\n",
    "\n",
    "                if i+1 < wave_map.shape[2]:\n",
    "                    diff_map[:,j,i] += diff_map[:,j,i]*(torch.matmul(key, wave_map[:, j, i+1]))#/wave_map[:, j, i+1].sum())\n",
    "\n",
    "        for k in range(wave_map.shape[0]):\n",
    "            for i in range(wave_map.shape[1]):\n",
    "                for j in range(wave_map.shape[2]):\n",
    "                    if diff_map[k,j,i] >= 4:\n",
    "                        diff_map[k,j,i] = 1\n",
    "                    else:\n",
    "                        diff_map[k,j,i] = 0\n",
    "\n",
    "\n",
    "        return diff_map\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Data loading block: Generate key, Generate maps, Diffuse maps\n",
    "\n",
    "\n",
    "\n",
    "df = diffusion()\n",
    "\n",
    "\n",
    "train_size = 100\n",
    "num_tiles = 13\n",
    "height = 6\n",
    "width = 6\n",
    "#key = gen_key(num_tiles, 6, 6)\n",
    "maps_training = torch.zeros(train_size, 50, num_tiles, height, width)\n",
    "diffusion_steps = torch.zeros(train_size)\n",
    "plt.imshow(key.squeeze().numpy())\n",
    "with torch.no_grad():\n",
    "    for m in range(train_size):\n",
    "        x, i_l = gen_map(key, height, width, num_tiles)\n",
    "        #for d in range(manifold_count):\n",
    "        df_unroll, itr = df.do_diffusion(x, key)\n",
    "\n",
    "        if itr < 50:\n",
    "            maps_training[m, 0:itr+1, :, :, :] = df_unroll.clone()\n",
    "            diffusion_steps[m] = itr\n",
    "\n",
    "        else:\n",
    "            print('whoops')\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD ------- DIFFUSION PROCESS ------ OLD\n",
    "\n",
    "\"\"\"\n",
    "Let's try to numerically diffuse the map into noise as the base DDPM method. \n",
    "Followed by \"undoing\" decisions systematically according to the surpise of a value and adding the tile\n",
    "to a companion wave space indicating tiles we COULD place if we so wanted to.\n",
    "0.0 Copy current collapsed map. This will be a binary map representing what can be placed.\n",
    "    1. when surpise goes below a threshold we \"undo\" one step of collapse:\n",
    "        1.2 Check legal spaces given \n",
    "    adding some value to it and all legal tiles in the wave space, then softmaxing it. \n",
    "    2. continue to diffuse with DDPM\n",
    "    3. Whenever a value goes below the surprise threshold, we add \n",
    "\"\"\"\n",
    "\n",
    "tau = torch.ones(1) * 100\n",
    "s = 0.008\n",
    "beta_schedule = torch.zeros((100))\n",
    "al =  torch.zeros((100))\n",
    "\n",
    "for t in range(100):\n",
    "\n",
    "    f1 = torch.cos(((t+1) / tau + s) / (1 + s) * torch.pi / 2) ** 2\n",
    "    f0 = torch.cos((t / tau + s) / (1 + s) * torch.pi / 2) ** 2\n",
    "\n",
    "    alpha = f1 / f0\n",
    "    al[t] = alpha\n",
    "\n",
    "    beta_schedule[t] = 1 - alpha\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def diffuse_uncertainty(x, beta, x_key, beta_time):\n",
    "    y = torch.rand_like(x) * x_key#\n",
    "    x = y*np.sqrt(1-beta) + x*np.sqrt(beta)\n",
    "    \n",
    "    for t in range(beta.shape[0]):\n",
    "        for j in range(beta.shape[1]):\n",
    "            for i in range(beta.shape[2]):\n",
    "                \n",
    "                if x_key[t,j,i] == 1:\n",
    "                    beta[t, j, i] = beta_schedule[beta_time[t, j, i]]\n",
    "                    beta_time[t, j, i] += 1\n",
    "\n",
    "    return x, torch.clamp(beta, 0, 1)\n",
    "\n",
    "def uncollapse(wave_map, key):\n",
    "    diff_map = torch.ones_like(wave_map)\n",
    "\n",
    "    for i in range(wave_map.shape[1]):\n",
    "        for j in range(wave_map.shape[2]):\n",
    "            \n",
    "            if j-1 >= 0:\n",
    "                diff_map[:,j,i] += diff_map[:,j,i]*(torch.matmul(key, wave_map[:, j-1, i]))#/wave_map[:, j-1, i].sum())\n",
    "\n",
    "            if j+1 < wave_map.shape[1]:\n",
    "                diff_map[:,j,i] += diff_map[:,j,i]*(torch.matmul(key, wave_map[:, j+1, i]))#/wave_map[:, j+1, i].sum())\n",
    "                    \n",
    "            if i-1 >= 0:\n",
    "                diff_map[:,j,i] += diff_map[:,j,i]*(torch.matmul(key, wave_map[:, j, i-1]))#/wave_map[:, j, i-1].sum())\n",
    "\n",
    "            if i+1 < wave_map.shape[2]:\n",
    "                diff_map[:,j,i] += diff_map[:,j,i]*(torch.matmul(key, wave_map[:, j, i+1]))#/wave_map[:, j, i+1].sum())\n",
    "\n",
    "\n",
    "\n",
    "    for k in range(wave_map.shape[0]):\n",
    "        for i in range(wave_map.shape[1]):\n",
    "            for j in range(wave_map.shape[2]):\n",
    "                if diff_map[k,j,i] >= 4:\n",
    "                    diff_map[k,j,i] = 1\n",
    "                else:\n",
    "                    diff_map[k,j,i] = 0\n",
    "\n",
    "\n",
    "    return diff_map\n",
    " \n",
    "#def information_uncertainty(x):\n",
    "\n",
    "def sf(x):\n",
    "    return torch.softmax(x, 0)\n",
    "\n",
    "def information(x, xo):\n",
    "    # input: wave vector\n",
    "    epsilon = 0.05\n",
    "    z = torch.ones(1) * x.shape[0]\n",
    "    max_ent = torch.sum(torch.log2(z))\n",
    "    entropy = torch.sum(-torch.log2(x)*x)\n",
    "\n",
    "    residual = torch.ones(xo.shape[0] - x.shape[0]) * epsilon\n",
    "    e_sans = torch.sum(-torch.log2(residual) * residual)\n",
    "\n",
    "    zo = torch.ones(1) * xo.shape[0]\n",
    "    max_ento = torch.sum(torch.log2(zo))\n",
    "    entropyo = torch.sum(-torch.log2(xo)*xo)\n",
    "\n",
    "    \n",
    "    ii = torch.ones(1) * (x.shape[0] / xo.shape[0])\n",
    "    ii_ = ii * -torch.log2(ii) + (1-ii) * -torch.log2(1-ii)\n",
    "\n",
    "    print('---- new, old',entropy, entropyo)\n",
    "\n",
    "\n",
    "    gain_ratio = (entropyo - entropy * ii - e_sans * (1 - ii)) / ii_\n",
    "\n",
    "    print('Entropy gain', entropyo - entropy * ii)\n",
    "    print('Gain ratio', gain_ratio)\n",
    "    print()\n",
    "    #return entropy\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def diffuse_wrapper(map_, key, thresh, step_size):\n",
    "\n",
    "\n",
    "    threshold = thresh\n",
    "    beta = torch.ones_like(map_)\n",
    "    beta_time = torch.zeros_like(beta)\n",
    "\n",
    "    x_key_mask = uncollapse(map_.clone(), key)\n",
    "    \n",
    "    x_key_threshold = map_.clone()\n",
    "    x_ = map_.clone()\n",
    "    map_unroll = map_.clone().unsqueeze(0)\n",
    "    iter = 0\n",
    "    while beta.sum() > 0:\n",
    "        cp = beta.sum()\n",
    "        x_, beta = diffuse_uncertainty(x_, beta, x_key_mask, beta_time)\n",
    "\n",
    "        for i in range(x_.shape[1]):\n",
    "            for j in range(x_.shape[2]):\n",
    "                entropy = torch.nonzero(x_[:,j,i])\n",
    "                s = entropy.shape[0]\n",
    "                entropy = torch.sum(-torch.log2((x_[entropy, j, i]))*(x_[entropy, j, i]))\n",
    "                \n",
    "                for k in range(x_.shape[0]):\n",
    "                    if x_[k,j,i] > 0:\n",
    "                        if (-torch.log2(x_[k,j,i])*x_[k,j,i] - entropy/s).abs() < threshold:\n",
    "                            # Key threshold controls the validity diffusion\n",
    "                            x_key_threshold[k,j,i] = 1\n",
    "\n",
    "        # Key mask controls the noise diffusion. Masks the Beta parameter \n",
    "        x_key_mask = uncollapse(x_key_threshold, key) \n",
    "\n",
    "        map_unroll = torch.concat((x_.unsqueeze(0), map_unroll), dim=0)\n",
    "        if beta.sum() == cp:\n",
    "            print('huh')\n",
    "            break\n",
    "    return map_unroll\n",
    "\n",
    "\n",
    "\n",
    "if 0:\n",
    "    sigma = 0.05**2\n",
    "    #x = torch.rand(5)\n",
    "    k1 = torch.zeros(10)\n",
    "    k2 = torch.zeros(10)\n",
    "\n",
    "    k1[0:3] = 1\n",
    "    k1[7:-1] = 1\n",
    "\n",
    "    k2[2:5] = 1\n",
    "\n",
    "    xn = torch.ones(10) * sigma\n",
    "    xn[0:5] = x\n",
    "    xn = sf(xn)\n",
    "\n",
    "    xk1 = xn*k1\n",
    "    xk2 = xn*k2\n",
    "    xk1 = (xk1[torch.nonzero(xk1)])\n",
    "    xk2 = (xk2[torch.nonzero(xk2)])\n",
    "\n",
    "    print(xk1)\n",
    "    print(xk2)\n",
    "\n",
    "    r1 = k1.sum() / 10\n",
    "    r2 = k2.sum() / 10\n",
    "\n",
    "    ii1 = -r1*torch.log2(r1) - (1-r1)*torch.log2((1-r1))\n",
    "    ii2 = -r2*torch.log2(r2) - (1-r2)*torch.log2((1-r2))\n",
    "\n",
    "    ek1 = torch.sum(-torch.log2(xk1)*xk1) / ii1\n",
    "    ek2 = torch.sum(-torch.log2(xk2)*xk2) / ii2\n",
    "\n",
    "    print(ek1, ek2, ii1, ii2)\n",
    "\n",
    "\n",
    "if 0:\n",
    "    print('key 4->6')\n",
    "    information( x, x_old)\n",
    "    print('key 4->8')\n",
    "    information( x, x_old_2)\n",
    "\n",
    "\n",
    "    print('key 4->6')\n",
    "    x_old_ = torch.ones(6) * 0.05\n",
    "    x_old_[0:4] = x.clone() * 0.3\n",
    "    information( x, x_old_)\n",
    "\n",
    "\n",
    "    print('key 4->8')\n",
    "    x_old_2_ = torch.ones(8) * 0.05\n",
    "    x_old_2_[0:4] = x.clone() * 0.3\n",
    "    information( x, x_old_2_)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key generated\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Binary key matrix')"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# OLD -------------- DATA LOADING --------- OLD\n",
    "\n",
    "num_tiles = 13\n",
    "wratio = 6\n",
    "hratio = 6\n",
    "map_h = 6\n",
    "map_w = 6\n",
    "entropy_thresh = 0.1\n",
    "beta = 0.05\n",
    "train_size = 10 # Number of maps in training dataset\n",
    "test_size = 1\n",
    "manifold_count = 4 # Number of different diffusions routes for each map \n",
    "diffusion_steps = 60\n",
    "diffusion_step_list = torch.zeros(train_size, manifold_count)\n",
    "map_20 = torch.zeros((train_size, manifold_count, diffusion_steps, num_tiles, map_h, map_w)) \n",
    "\n",
    "key_valid = False\n",
    "while key_valid == False:\n",
    "    key = gen_key(num_tiles, wratio, hratio)\n",
    "    key_valid = True\n",
    "    for i in range(key.shape[0]):\n",
    "        if key[i,:].sum() == 0:\n",
    "            key_valid = False\n",
    "\n",
    "print('Key generated')\n",
    "with torch.no_grad():\n",
    "    for k in range(train_size):\n",
    "        \n",
    "        map_f, id_map = gen_map(key, map_h, map_w, num_tiles)\n",
    "\n",
    "        for m in range(manifold_count):\n",
    "            map_ = diffuse_wrapper(map_f.clone(), key, entropy_thresh, beta)\n",
    "            #index = np.linspace(0,map_.shape[0]-1, diffusion_steps)\n",
    "            diffusion_step_list[k, m] = map_.shape[0]\n",
    "            map_20[k, m, 0:map_.shape[0]] = map_\n",
    "\n",
    "print()\n",
    "\n",
    "plt.figure(1)\n",
    "plt.imshow(key.numpy())\n",
    "plt.title('Binary key matrix')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed xyz: tensor([4]) tensor([12]) tensor([[6]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d6/9_0p0ctd5cd5q3f8lhpbzpc40000gn/T/ipykernel_67184/2224842711.py:10: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  wave_map = diffuse(wave_map, z//width, z%width, key, width, height)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# PROTOTYPING\n",
    "\n",
    "num_tiles = 13\n",
    "\n",
    "width = 16\n",
    "height = width \n",
    "\n",
    "h = 5\n",
    "w = 5\n",
    "map_init = torch.randint(0, num_tiles, (h, w)) # [H, W]\n",
    "map_init = (map_init)\n",
    "\n",
    "#  Key format: KEY[VALUE, QUERY] -> query is neighbor, value is us tile, returns validity of us given neighbor\n",
    "\n",
    "if 1:\n",
    "    key = torch.zeros(( num_tiles, num_tiles))# [1, N, N]\n",
    "    for j in range(w):\n",
    "        for i in range(h):\n",
    "            \n",
    "            if i+1 < h:\n",
    "                key[map_init[i, j], map_init[i+1, j]] = 1 \n",
    "            if i-1 >= 0:\n",
    "                key[map_init[i, j], map_init[i-1, j]] = 1 \n",
    "\n",
    "            if j+1 < w:\n",
    "                key[map_init[i, j], map_init[i, j+1]] = 1 \n",
    "            if j-1 >= 0:\n",
    "                key[map_init[i, j], map_init[i, j-1]] = 1 \n",
    "\n",
    "\n",
    "    key = key #torch.clamp(key - torch.eye(num_tiles).squeeze(), 0, 1)\n",
    "\n",
    "if 0:\n",
    "    plt.figure(1)\n",
    "    plt.imshow(key.squeeze().numpy())\n",
    "    plt.title('WFC key')\n",
    "\n",
    "if 1:\n",
    "    seed = torch.ones((num_tiles, width, height))\n",
    "    xy = torch.randint(0, height, (2,1))\n",
    "    #xy[1] = 0\n",
    "    #xy[0] = 1\n",
    "    z = torch.randint(0, num_tiles, (1,1))\n",
    "    seed[:,xy[0], xy[1]] = seed[:,xy[0], xy[1]]*0\n",
    "    seed[z,xy[0], xy[1]] = 1\n",
    "\n",
    "    print('seed xyz:', xy[1], xy[0], z)\n",
    "\n",
    "\n",
    "\n",
    "map_ = actual_wfc(num_tiles, width, height, key, seed, xy[1]+xy[0]*width).squeeze()\n",
    "\n",
    "val, id = torch.max(map_.flatten(-2,-1),dim=0)\n",
    "id_map = id.reshape(height,width)\n",
    "\n",
    "valid, vmap = validator(id_map, key)\n",
    "print(valid)\n",
    "\n",
    "plt.figure(1)\n",
    "plt.imshow(key)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.figure(2)\n",
    "plt.imshow(id_map.squeeze().numpy())\n",
    "plt.title('WFC map')\n",
    "plt.colorbar()\n",
    "\n",
    "if 0:\n",
    "    plt.figure(3)\n",
    "    plt.imshow(id_map_old.squeeze().numpy())\n",
    "    plt.title('WFC map old')\n",
    "    plt.colorbar()\n",
    "\n",
    "\n",
    "    id_map_old = id_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([[7, 0, 1],\n",
      "        [1, 0, 1],\n",
      "        [0, 5, 6]])\n",
      "tensor([[1, 1, 6, 2, 0],\n",
      "        [5, 2, 6, 7, 6],\n",
      "        [7, 7, 4, 1, 6],\n",
      "        [2, 6, 1, 0, 5],\n",
      "        [5, 7, 3, 5, 0]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x7fa6a20c1ca0>"
      ]
     },
     "execution_count": 537,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# PROTOTYPING \n",
    "\n",
    "def post_process(id_map, v_map, key):\n",
    "    id_vec = torch.arange(key.shape[0])\n",
    "    for threshold in range(4):\n",
    "        for i in range(id_map.shape[0]):\n",
    "            for j in range(id_map.shape[1]):\n",
    "                if vmap[j, i] == threshold - 3:\n",
    "                    t = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(key[7,:])\n",
    "\n",
    "valid, vmap = validator(id_map, key)\n",
    "\n",
    "print(id_map[0:3,0:3])\n",
    "\n",
    "plt.imshow(vmap)\n",
    "plt.colorbar()\n",
    "\n",
    "print(map_init)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 6, 2, 0],\n",
      "        [5, 2, 6, 7, 6],\n",
      "        [7, 7, 4, 1, 6],\n",
      "        [2, 6, 1, 0, 5],\n",
      "        [5, 7, 3, 5, 0]])\n",
      "\n",
      "\n",
      " ij 1 , 0 map val tensor(5)\n",
      "tensor([0., 1., 0., 0., 0., 1., 0., 0.])\n",
      " ij 1 , 1 map val tensor(2)\n",
      "tensor([0., 1., 1., 0., 0., 1., 1., 0.])\n",
      " ij 1 , 2 map val tensor(6)\n",
      "tensor([0., 1., 1., 0., 0., 1., 1., 0.])\n",
      " ij 1 , 3 map val tensor(7)\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 0.])\n",
      " ij 1 , 4 map val tensor(6)\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor(6) tensor(0)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x7fa6a10d4490>"
      ]
     },
     "execution_count": 542,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# PROTOTYPING \n",
    "\n",
    "print(map_init)\n",
    "print()\n",
    "print()\n",
    "key2 = torch.zeros_like(key)\n",
    "for j in range(w):\n",
    "        for i in range(h):\n",
    "\n",
    "\n",
    "            if i+1 < h:\n",
    "                key2[map_init[i, j], map_init[i+1, j]] = 1 \n",
    "            if i-1 >= 0:\n",
    "                key2[map_init[i, j], map_init[i-1, j]] = 1 \n",
    "\n",
    "            if j+1 < w:\n",
    "                key2[map_init[i, j], map_init[i, j+1]] = 1 \n",
    "            if j-1 >= 0:\n",
    "                key2[map_init[i, j], map_init[i, j-1]] = 1 \n",
    "\n",
    "            if i == 1:\n",
    "                 print(' ij', i, ',',j, 'map val', map_init[i,j])\n",
    "                 print(key2[i,:])\n",
    "                 if j == 4:\n",
    "                      print(map_init[i, j], map_init[i-1, j])\n",
    "print()\n",
    "\n",
    "plt.figure(1)\n",
    "plt.imshow(key2)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training dataset has a mean of  tensor(0.) and var of  tensor(0.)\n",
      "\n",
      "Loss for epoch 19 : 7.619708775052914e-08 with ratio of  tensor(0.0024)\n",
      "Percent valid testing noise tensor(2.0000) Variance of tensor(0.0016) and a mean tile choice of  tensor(7.2496)\n",
      "\n",
      "\n",
      "Loss for epoch 39 : 2.20693530311733e-08 with ratio of  tensor(0.)\n",
      "Percent valid testing noise tensor(0.) Variance of tensor(0.) and a mean tile choice of  tensor(3.)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-83-5538967be481>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    115\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m             \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap_wfc_retained\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m             \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m             \u001b[0mv_ratio\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m100.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-4c73b9f82408>\u001b[0m in \u001b[0;36mvalidator\u001b[1;34m(x, key)\u001b[0m\n\u001b[0;32m    125\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m                     \u001b[0mvalid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m                     \u001b[0mvmap\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TRAIN LOOP\n",
    "\n",
    "epochs = 100\n",
    "width = 10 #map_w\n",
    "height = 10 #map_h\n",
    "num_tiles = num_tiles\n",
    "diffusion_steps_GRU = 20\n",
    "kernel_size = 3\n",
    "l_sum_np = 0\n",
    "batch_size = 20\n",
    "loss = 0\n",
    "\n",
    "index_list = np.arange(train_size)\n",
    "\n",
    "gru = GRU_WFC(num_tiles, 0, height, width, diffusion_steps_GRU, kernel_size, bias_scale = 0.25, deep = False)\n",
    "\n",
    "loss_func =  nn.MSELoss() # nn.L1Loss() #\n",
    "l_kl = torch.nn.KLDivLoss(reduction=\"batchmean\", log_target = True)\n",
    "lbce = nn.BCEWithLogitsLoss()\n",
    "\n",
    "#loss_l1 = nn.L1Loss()\n",
    "optimizer = torch.optim.NAdam(gru.parameters(), lr = 0.001)\n",
    "\n",
    "map_ = torch.zeros(train_size, 13, height, width)\n",
    "for ii in range(train_size):\n",
    "    map_[ii] = maps_training[ii, int(diffusion_steps[ii]), :, :, :]\n",
    "\n",
    "\n",
    "_, id_set = torch.max(map_, dim = 1)\n",
    "\n",
    "va, m = (torch.var_mean(id_set.float(), 0))\n",
    "print('The training dataset has a mean of ', m.mean(),'and var of ', va.mean())\n",
    "\n",
    "\n",
    "#truth_h = torch.zeros((height-1)*width*num_tiles)\n",
    "#truth_v = torch.zeros(height*(width-1)*num_tiles)\n",
    "\n",
    "\n",
    "#truth_h = torch.zeros(1)\n",
    "#truth_v = torch.zeros(1)\n",
    "\n",
    "#key = torch.zeros(( num_tiles, num_tiles))# [1, N, N]\n",
    "#map_init = torch.randint(0, num_tiles, (height, width)) # [H, W]\n",
    "\n",
    "#map_init = torch.randint(1, num_tiles, (8, 8)) # [H, W]\n",
    "#map_init = pad(map_init)\n",
    "#cancel = 1-torch.eye(num_tiles)\n",
    "\n",
    "if 0:\n",
    "    for i in range(8):\n",
    "        for j in range(8):\n",
    "            \n",
    "            if i+1 < height:\n",
    "                key[map_init[i, j], map_init[i+1, j]] = 1 \n",
    "            if i-1 > 0:\n",
    "                key[map_init[i, j], map_init[i-1, j]] = 1 \n",
    "\n",
    "            if j+1 < width:\n",
    "                key[map_init[i, j], map_init[i, j+1]] = 1 \n",
    "            if j-1 > 0:\n",
    "                key[map_init[i, j], map_init[i, j-1]] = 1 \n",
    "\n",
    "#key = key*cancel\n",
    "#key = torch.ones((2,2)) - torch.eye(2)\n",
    "\n",
    "#gru_mask_init = torch.ones(num_tiles, height, width)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loss = 0\n",
    "    v_ratio = 0\n",
    "    np.random.shuffle(index_list)\n",
    "\n",
    "    for i1 in range(train_size):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # map_20 = torch.zeros((train_size, manifold_count, diffusion_steps, num_tiles, map_h, map_w)) \n",
    "            id = index_list[i1]\n",
    "            dfinal = diffusion_steps[id]\n",
    "            h_init = maps_training[id, 0:int(dfinal)+1]\n",
    "            \n",
    "\n",
    "            #h_init = torch.rand((1, num_tiles, height, width))\n",
    "            #h_init = torch.softmax(h_init, dim=1)\n",
    "            #x = torch.randint(1, width-2, (1,1))\n",
    "            #y = torch.randint(1, height-2, (1,1))\n",
    "            #v = torch.randint(0, num_tiles-1, (1,1))\n",
    "            #h_init[0, :, y, x] = h_init[0, :, y, x] * 0\n",
    "            #h_init[0, v, y, x] = h_init[0, v, y, x] + 1\n",
    "            #gru.mask = gru_mask_init.clone()\n",
    "            \n",
    "        #gru.mask[:, x, y] = gru.mask[:, x, y] * 0\n",
    "        #gru.mask[v, x, y] = gru.mask[v, x, y] + 1\n",
    "\n",
    "        h_init = h_init.requires_grad_(True)\n",
    "        \n",
    "\n",
    "        #for m in range(4):\n",
    "        fmap = h_init[0].clone().squeeze().unsqueeze(0)\n",
    "        \n",
    "        if 0:\n",
    "            map_wfc_retained = gru.forward(fmap, dfinal, training = True).squeeze() #int(dfinal[m]), training = False).squeeze()\n",
    "            loss += loss_func(map_wfc_retained, h_init[1:int(dfinal)])\n",
    "            #loss += l_kl(torch.log_softmax(map_wfc_retained.squeeze(), 0), torch.log_softmax(h_init[int(dfinal)].squeeze(), 0)) \n",
    "            loss += lbce(torch.log_softmax(map_wfc_retained.transpose(-2,-1).squeeze(), 0), h_init[1:int(dfinal)].transpose(-2,-1).squeeze()) / 2\n",
    "            #loss_v, loss_vd, loss_h, loss_hl = wfc_loss_2(map_wfc, key)\n",
    "        if 1:\n",
    "            \n",
    "            map_wfc_retained = gru.forward(fmap, 10, training = False).squeeze() #int(dfinal[m]), training = False).squeeze()\n",
    "            \n",
    "            loss += loss_func(map_wfc_retained, h_init[int(dfinal)])\n",
    "            #loss += l_kl(torch.log_softmax(map_wfc_retained.squeeze(), 0), torch.log_softmax(h_init[int(dfinal)].squeeze(), 0)) \n",
    "            #loss += lbce(torch.log_softmax(map_wfc_retained.flatten(-2,-1).squeeze(), 0), h_init[int(dfinal)].flatten(-2,-1).squeeze()) / 2\n",
    "            #loss_v, loss_vd, loss_h, loss_hl = wfc_loss_2(map_wfc, key)    \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            _, id_pred = torch.max(map_wfc_retained.clone().detach(), dim = 0)\n",
    "            _, v = validator(id_pred, key)\n",
    "            v_ratio += torch.clamp(v, 0, 1).sum()/36.\n",
    "            \n",
    "        #loss_v = loss_v.flatten()\n",
    "        #loss_vd = loss_vd.flatten()\n",
    "        #loss_h = loss_h.flatten()\n",
    "        #loss_hl = loss_hl.flatten()\n",
    "        \n",
    "        #l_p = loss_l1(map_wfc.squeeze(), map_test.squeeze())# (loss_func(loss_v, truth_v) + loss_func(loss_h, truth_h)+ loss_func(loss_hl, truth_h) + loss_func(loss_vd, truth_v)) / 4\n",
    "    \n",
    "        # map_20[0,0, int(diffusion_step_list[0,0]-1)]\n",
    "\n",
    "        \n",
    "        l_sum_np += loss.clone().detach().numpy()\n",
    "        if 1:\n",
    "            loss = loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            del loss\n",
    "            loss = 0\n",
    "    \n",
    "    if (epoch+1)%20 == 0:\n",
    "        vsum = torch.zeros(100, height, width)\n",
    "        idsum = torch.zeros(100, height, width)\n",
    "        for k in range(100):\n",
    "            h_noise = torch.rand_like(h_init[0].clone().squeeze().unsqueeze(0))\n",
    "            map_wfc_noise = gru.forward(h_noise, 20, training = False).squeeze()\n",
    "            _, id_noise = torch.max(map_wfc_noise, dim = 0)\n",
    "\n",
    "            _, v = validator(id_noise, key)\n",
    "            idsum[k] = id_noise\n",
    "            vsum[k] = v\n",
    "            \n",
    "        va, me = torch.var_mean(idsum, dim = 0)\n",
    "        print()\n",
    "        print('Loss for epoch', epoch,':', l_sum_np / (train_size), 'with ratio of ', v_ratio / train_size)\n",
    "        print('Percent valid testing noise', torch.clamp(vsum/100, 0, 1).sum(), 'Variance of', va.mean(), 'and a mean tile choice of ', me.mean())        \n",
    "        print()\n",
    "    l_sum_np = 0\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 50, 13, 6, 6])\n",
      "torch.Size([100, 13, 6, 6])\n",
      "torch.Size([100, 6, 6])\n",
      "Mean of  tensor(6.5692) and var of  tensor(15.0661)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(maps_training.shape)\n",
    "map_ = torch.zeros(100, 13, height, width)\n",
    "for ii in range(100):\n",
    "    map_[ii] = maps_training[ii, int(diffusion_steps[ii]), :, :, :]\n",
    "\n",
    "\n",
    "_, id_set = torch.max(map_, dim = 1)\n",
    "\n",
    "va, m = (torch.var_mean(id_set.float(), 0))\n",
    "print('The training dataset has a mean of ', m.mean(),'and var of ', va.mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Best so far 500 train set 13 tiles 63 kb')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#print(maps_training.shape, h_init[0])\n",
    "    #h_init[1, 0, :, :, :])\n",
    "\n",
    "#PATH = 'gru_best_so_far_500trainset'\n",
    "#torch.save(gru.state_dict(), PATH)\n",
    "\n",
    "#print(h_init.shape)\n",
    "plt.figure(1)\n",
    "plt.imshow(key.squeeze().numpy())\n",
    "plt.title('Best so far 500 train set 13 tiles ---- kb')\n",
    "\n",
    "#print(torch.log_softmax(h_init[0], 0))\n",
    "# 20 epochs dfinal + 10 = 0.025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total valid points tensor(0)  out of 36000  =  tensor(0.)  percent correctness\n",
      "tensor(0., grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#gru = GRU_WFC(num_tiles, 0, height, width, diffusion_steps_GRU, bias_scale = 0.25, deep = True)\n",
    "\n",
    "if 0:\n",
    "    with torch.no_grad():\n",
    "        i1 = 88\n",
    "        dfinal = diffusion_steps[i1]\n",
    "        h_init = maps_training[i1, 0:int(dfinal)+1] \n",
    "        h_noise = torch.rand_like(h_init[0].clone().squeeze().unsqueeze(0))\n",
    "        #print(h_init.shape, maps_training.shape)\n",
    "\n",
    "        map_wfc_test = gru.forward(h_init[0].clone().squeeze().unsqueeze(0), 10, training = False).squeeze()\n",
    "        map_wfc_noise = gru.forward(h_noise, 20, training = False).squeeze()\n",
    "        \n",
    "        print()\n",
    "        print()\n",
    "        print('map sum', map_wfc_test.sum())\n",
    "        print('shapes', map_wfc_test.shape, h_init.shape)\n",
    "        _, id_pred = torch.max(map_wfc_test, dim = 0)\n",
    "        _, id_noise = torch.max(map_wfc_noise, dim = 0)\n",
    "        _, id_gt = torch.max(h_init[h_init.shape[0]-1], dim = 0)\n",
    "        _, v = validator(id_pred, key)\n",
    "        print('predicted map')\n",
    "        print(id_pred)\n",
    "        print('Validity map')\n",
    "        print(v.sum())\n",
    "        print('rounded error:', (id_pred - id_gt).abs().sum())\n",
    "\n",
    "        print(map_wfc_test[:, -1, -1])\n",
    "        print(h_init[15,:,-1,-1])\n",
    "\n",
    "        vsum = 0\n",
    "\n",
    "v_sum = 0\n",
    "width = 6\n",
    "height = width\n",
    "g = 0\n",
    "#id_nl = torch.zeros((1000, height, width))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for it in range(1):\n",
    "        for k in range(100):\n",
    "            h_noise = torch.rand((1, 13, height, width))\n",
    "            map_wfc_noise = gru.forward(h_noise, 70, training = False).squeeze()\n",
    "            _, id_noise = torch.max(map_wfc_noise, dim = 0)\n",
    "\n",
    "            valid, v = validator(id_noise, key)\n",
    "            v = torch.clamp(v, 0, 1).sum()\n",
    "            v_sum += v\n",
    "\n",
    "    \n",
    "    #v_sum = (v_sum/(width*height))\n",
    "\n",
    "#print(g,' good maps out of ', it+1)\n",
    "#var, me= torch.var_mean(id_nl, dim=0)\n",
    "#print('mean ', me.mean(), me.std(), ' and variance ', var.mean(), var.std())\n",
    "print('Total valid points', v_sum, ' out of', 1000*36,' = ', v_sum/(1000*36),' percent correctness')\n",
    "print(h_init.sum())\n",
    "# https://arxiv.org/pdf/2102.09672.pdf\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.1000000000000005  ms per 6x6 map with 13 tiles and 20 refinement steps. Using DEAD Net small (63 kb)\n",
      "96.22999999999999 ms for 10,000 32x32 with 13 tiles\n",
      "\n",
      "True\n",
      "tensor(1.) 0.9444444444444444\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "print(71/10000*1000,' ms per 6x6 map with 13 tiles and 20 refinement steps. Using DEAD Net small (63 kb)')\n",
    "print((16*60+2.3)/10000*1000, 'ms for 10,000 32x32 with 13 tiles')\n",
    "print()\n",
    "p, v = validator(id_noise, key)\n",
    "print(p)\n",
    "\n",
    "print(v.sum()/(height * width), 34/36)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nnn tensor(0.7408)\n",
      "tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor(0.7408)\n",
      "tensor(0.2245)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "l_kl = torch.nn.KLDivLoss(reduction=\"batchmean\", log_target = True)\n",
    "lbce = torch.nn.BCELoss()\n",
    "lbcelog = nn.BCEWithLogitsLoss(reduction=\"mean\")\n",
    "\n",
    "\n",
    "\n",
    "p = torch.zeros(13, 5, 5)\n",
    "p[1,:,:] = 1\n",
    "\n",
    "t = torch.zeros(13, 5, 5)\n",
    "t[2,:,:] = 1\n",
    "\n",
    "p2 = torch.zeros(13)\n",
    "t2 = p2.clone()\n",
    "p2[1] = 1\n",
    "t2[2] = 1\n",
    "print('nnn', lbcelog(p2, t.flatten(-2,-1)[:,0].squeeze()))\n",
    "\n",
    "print(p.flatten(-2,-1)[:,0], t.flatten(-2,-1)[:,0])\n",
    "print(p.flatten(-2,-1)[:,0], p2)\n",
    "print(lbcelog(p.flatten(-2,-1)[:,0],t.flatten(-2,-1)[:,0]))\n",
    "\n",
    "p = torch.log_softmax(p,0)\n",
    "t = torch.log_softmax(t,0)\n",
    "\n",
    "#print(p, t)\n",
    "print(l_kl(p, t))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'h' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-4c92aa68b561>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_key\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_tiles\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi_l\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_tiles\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[0mmap_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdiffuse_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.025\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'h' is not defined"
     ]
    }
   ],
   "source": [
    "# DIFFUSION PROCESS\n",
    "\n",
    "\"\"\"\n",
    "Let's try to numerically diffuse the map into noise as the base DDPM method. \n",
    "Followed by \"undoing\" decisions systematically according to the surpise of a value and adding the tile\n",
    "to a companion wave space indicating tiles we COULD place if we so wanted to.\n",
    "0.0 Copy current collapsed map. This will be a binary map representing what can be placed.\n",
    "    1. when surpise goes below a threshold we \"undo\" one step of collapse:\n",
    "        1.2 Check legal spaces given \n",
    "    adding some value to it and all legal tiles in the wave space, then softmaxing it. \n",
    "    2. continue to diffuse with DDPM\n",
    "    3. Whenever a value goes below the surprise threshold, we add \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def diffuse_uncertainty(x, beta, x_key, step):\n",
    "    y = torch.rand_like(x) * x_key#\n",
    "    x = y*np.sqrt(1-beta) + x*np.sqrt(beta)\n",
    " \n",
    "    beta += -step* x_key\n",
    "\n",
    "    return x, torch.clamp(beta, 0, 1)\n",
    "\n",
    "def uncollapse(wave_map, key):\n",
    "    diff_map = torch.ones_like(wave_map)\n",
    "\n",
    "    for i in range(wave_map.shape[1]):\n",
    "        for j in range(wave_map.shape[2]):\n",
    "            \n",
    "            if j-1 >= 0:\n",
    "                diff_map[:,j,i] += diff_map[:,j,i]*(torch.matmul(key, wave_map[:, j-1, i]))#/wave_map[:, j-1, i].sum())\n",
    "\n",
    "            if j+1 < x.shape[1]:\n",
    "                diff_map[:,j,i] += diff_map[:,j,i]*(torch.matmul(key, wave_map[:, j+1, i]))#/wave_map[:, j+1, i].sum())\n",
    "                    \n",
    "            if i-1 >= 0:\n",
    "                diff_map[:,j,i] += diff_map[:,j,i]*(torch.matmul(key, wave_map[:, j, i-1]))#/wave_map[:, j, i-1].sum())\n",
    "\n",
    "            if i+1 < x.shape[2]:\n",
    "                diff_map[:,j,i] += diff_map[:,j,i]*(torch.matmul(key, wave_map[:, j, i+1]))#/wave_map[:, j, i+1].sum())\n",
    "\n",
    "\n",
    "\n",
    "    for k in range(wave_map.shape[0]):\n",
    "        for i in range(wave_map.shape[1]):\n",
    "            for j in range(wave_map.shape[2]):\n",
    "                if diff_map[k,j,i] >= 4:\n",
    "                    diff_map[k,j,i] = 1\n",
    "                else:\n",
    "                    diff_map[k,j,i] = 0\n",
    "\n",
    "\n",
    "    return diff_map\n",
    " \n",
    "#def information_uncertainty(x):\n",
    "\n",
    "def sf(x):\n",
    "    return torch.softmax(x, 0)\n",
    "\n",
    "def information(x, xo):\n",
    "    # input: wave vector\n",
    "    epsilon = 0.05\n",
    "    z = torch.ones(1) * x.shape[0]\n",
    "    max_ent = torch.sum(torch.log2(z))\n",
    "    entropy = torch.sum(-torch.log2(x)*x)\n",
    "\n",
    "    residual = torch.ones(xo.shape[0] - x.shape[0]) * epsilon\n",
    "    e_sans = torch.sum(-torch.log2(residual) * residual)\n",
    "\n",
    "    zo = torch.ones(1) * xo.shape[0]\n",
    "    max_ento = torch.sum(torch.log2(zo))\n",
    "    entropyo = torch.sum(-torch.log2(xo)*xo)\n",
    "\n",
    "    \n",
    "    ii = torch.ones(1) * (x.shape[0] / xo.shape[0])\n",
    "    ii_ = ii * -torch.log2(ii) + (1-ii) * -torch.log2(1-ii)\n",
    "\n",
    "    print('---- new, old',entropy, entropyo)\n",
    "\n",
    "\n",
    "    gain_ratio = (entropyo - entropy * ii - e_sans * (1 - ii)) / ii_\n",
    "\n",
    "    print('Entropy gain', entropyo - entropy * ii)\n",
    "    print('Gain ratio', gain_ratio)\n",
    "    print()\n",
    "    #return entropy\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def diffuse_wrapper(map_, key, thresh, step_size):\n",
    "\n",
    "\n",
    "    threshold = thresh\n",
    "    beta = torch.ones_like(map_)\n",
    "\n",
    "    x_key_mask = uncollapse(map_.clone(), key)\n",
    "    \n",
    "    x_key_threshold = map_.clone()\n",
    "    x_ = map_.clone()\n",
    "    map_unroll = map_.clone().unsqueeze(0)\n",
    "    iter = 0\n",
    "    while beta.sum() > 0:\n",
    "        cp = beta.sum()\n",
    "        x_, beta = diffuse_uncertainty(x_, beta, x_key_mask, step_size)\n",
    "\n",
    "        for i in range(x_.shape[1]):\n",
    "            for j in range(x_.shape[2]):\n",
    "                entropy = torch.nonzero(x_[:,j,i])\n",
    "                s = entropy.shape[0]\n",
    "                entropy = torch.sum(-torch.log2((x_[entropy, j, i]))*(x_[entropy, j, i]))\n",
    "                \n",
    "                for k in range(x_.shape[0]):\n",
    "                    if x_[k,j,i] > 0:\n",
    "                        if (-torch.log2(x_[k,j,i])*x_[k,j,i] - entropy/s).abs() < threshold:\n",
    "                            # Key threshold controls the validity diffusion\n",
    "                            x_key_threshold[k,j,i] = 1\n",
    "\n",
    "        # Key mask controls the noise diffusion. Masks the Beta parameter \n",
    "        x_key_mask = uncollapse(x_key_threshold, key) \n",
    "\n",
    "        map_unroll = torch.concat((map_unroll, x_.unsqueeze(0)), dim=0)\n",
    "        if beta.sum() == cp:\n",
    "            print('huh')\n",
    "            break\n",
    "    return map_unroll\n",
    "\n",
    "\n",
    "\n",
    "key = gen_key(num_tiles, 6, 6)\n",
    "x, i_l = gen_map(key, h, w, num_tiles)\n",
    "\n",
    "map_train = diffuse_wrapper(x, key, 0.15, 0.025)\n",
    "\n",
    "print(map_train.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#print(d)\n",
    "\n",
    "#x = torch.rand(4)\n",
    "#x[1:4] = x[1:4] \n",
    "\n",
    "#y = x.clone()\n",
    "#x[:-1] = torch.rand(3) * 0.2\n",
    "\n",
    "#print('inp', x)\n",
    "\n",
    "\n",
    "if 0:\n",
    "    sigma = 0.05**2\n",
    "    #x = torch.rand(5)\n",
    "    k1 = torch.zeros(10)\n",
    "    k2 = torch.zeros(10)\n",
    "\n",
    "    k1[0:3] = 1\n",
    "    k1[7:-1] = 1\n",
    "\n",
    "    k2[2:5] = 1\n",
    "\n",
    "    xn = torch.ones(10) * sigma\n",
    "    xn[0:5] = x\n",
    "    xn = sf(xn)\n",
    "\n",
    "    xk1 = xn*k1\n",
    "    xk2 = xn*k2\n",
    "    xk1 = (xk1[torch.nonzero(xk1)])\n",
    "    xk2 = (xk2[torch.nonzero(xk2)])\n",
    "\n",
    "    print(xk1)\n",
    "    print(xk2)\n",
    "\n",
    "    r1 = k1.sum() / 10\n",
    "    r2 = k2.sum() / 10\n",
    "\n",
    "    ii1 = -r1*torch.log2(r1) - (1-r1)*torch.log2((1-r1))\n",
    "    ii2 = -r2*torch.log2(r2) - (1-r2)*torch.log2((1-r2))\n",
    "\n",
    "    ek1 = torch.sum(-torch.log2(xk1)*xk1) / ii1\n",
    "    ek2 = torch.sum(-torch.log2(xk2)*xk2) / ii2\n",
    "\n",
    "    print(ek1, ek2, ii1, ii2)\n",
    "\n",
    "\n",
    "if 0:\n",
    "    print('key 4->6')\n",
    "    information( x, x_old)\n",
    "    print('key 4->8')\n",
    "    information( x, x_old_2)\n",
    "\n",
    "\n",
    "    print('key 4->6')\n",
    "    x_old_ = torch.ones(6) * 0.05\n",
    "    x_old_[0:4] = x.clone() * 0.3\n",
    "    information( x, x_old_)\n",
    "\n",
    "\n",
    "    print('key 4->8')\n",
    "    x_old_2_ = torch.ones(8) * 0.05\n",
    "    x_old_2_[0:4] = x.clone() * 0.3\n",
    "    information( x, x_old_2_)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum entropy tensor([-0.1557, -0.7224, -0.2991, -0.3245]) tensor([-0.4776, -1.0443, -0.6210, -0.6465])\n",
      " cummulative entropy tensor(nan)\n",
      "z low tensor([[[1., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [1., 0., 0.],\n",
      "         [0., 0., 1.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 1., 0.]]]) z high tensor([[[0., 0., 0.],\n",
      "         [1., 0., 0.],\n",
      "         [0., 0., 1.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 1., 0.]],\n",
      "\n",
      "        [[0., 1., 0.],\n",
      "         [0., 0., 1.],\n",
      "         [0., 0., 0.]]])\n",
      " probabilities tensor([[[1., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [1., 0., 0.],\n",
      "         [0., 0., 1.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 1., 0.]],\n",
      "\n",
      "        [[0., 1., 0.],\n",
      "         [0., 0., 1.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 1.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [1., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 1., 0.],\n",
      "         [0., 0., 0.]]]) tensor([[[1., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [1., 0., 0.],\n",
      "         [0., 0., 1.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 1., 0.]]]) tensor([[[0., 0., 0.],\n",
      "         [1., 0., 0.],\n",
      "         [0., 0., 1.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.],\n",
      "         [0., 1., 0.]],\n",
      "\n",
      "        [[0., 1., 0.],\n",
      "         [0., 0., 1.],\n",
      "         [0., 0., 0.]]])\n",
      "GINI pre tensor(-8.)\n",
      "GINI collapse low tensor(-3.)\n",
      "GINI collapse high tensor(-4.)\n",
      " delta E tensor(nan) tensor(nan) tensor(nan)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#x = torch.softmax(torch.randn(5),0)\n",
    "#y = torch.ones(1)*5\n",
    "print('maximum entropy', -torch.log2(1/y), -torch.log2(1/y*(5/4)))\n",
    "\n",
    "print(' cummulative entropy', torch.sum(-torch.log2(x)*x))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "z = torch.ones(4)\n",
    "z = x[0:4].clone()\n",
    "zhigh = x[1:5].clone()\n",
    "print('z low', z, 'z high', zhigh)\n",
    "#z = torch.softmax(z,0)\n",
    "#zhigh = torch.softmax(zhigh,0)\n",
    "\n",
    "\n",
    "print(' probabilities', x, z, zhigh)\n",
    "print('GINI pre', 1 - torch.sum(x**2))\n",
    "print('GINI collapse low', 1 - torch.sum(z**2))\n",
    "print('GINI collapse high', 1 - torch.sum(zhigh**2))\n",
    "\n",
    "\n",
    "print(' delta E', torch.sum(-torch.log2(x)*x), torch.sum(-torch.log2(z)*z),  torch.sum(-torch.log2(zhigh)*zhigh))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splits tensor([0.0500, 0.0500]) tensor([0.7000, 0.2000, 0.0500])\n",
      "tensor(0.4322) tensor(1.0407)\n"
     ]
    }
   ],
   "source": [
    "# INFORMATION GAIN EXPERIMENTS\n",
    "\n",
    "def entropy_gain(x, query, y):\n",
    "    t = 0\n",
    "\n",
    "\n",
    "p = torch.ones(4)\n",
    "p[0] = 0.7\n",
    "p[1] = 0.2\n",
    "p[2] = 0.05\n",
    "p[3] = 0.05\n",
    "\n",
    "p1 = p[2:4]\n",
    "p1rem = p[0:2]\n",
    "p2 = p[0:3]\n",
    "p2rem = p[3]\n",
    "print('splits', p1, p2)\n",
    "\n",
    "\n",
    "sup1 = torch.sum(torch.log2(1/p1)*p1)\n",
    "sup2 = torch.sum(torch.log2(1/p2)*p2)\n",
    "\n",
    "print(sup1, sup2)\n",
    "\n",
    "\n",
    "if 0:\n",
    "\n",
    "    r1 = p1.shape[0]/p.shape[0]\n",
    "    r2 = p2.shape[0]/p.shape[0]\n",
    "\n",
    "    ent = torch.sum(-torch.log2(p)*p)\n",
    "    ep1 = torch.sum(-torch.log2(p1)*p1)  #- (1-r1)*torch.sum(p1rem * torch.log2(p1rem))*0\n",
    "    ep2 = torch.sum(-torch.log2(p2)*p2)  #- (1-r2)*torch.sum(p2rem * torch.log2(p2rem))*0\n",
    "\n",
    "    d = torch.ones(1) * p.shape[0]\n",
    "\n",
    "    d1 = -r1*torch.log2(p1.shape[0]/d) #- (1-r1)*torch.log2(1-p1.shape[0]/d)\n",
    "    d2 = -r2*torch.log2(p2.shape[0]/d) #- (1-r2)*torch.log2(1-p2.shape[0]/d)\n",
    "\n",
    "\n",
    "    ig1 = (ent - ep1 * r1)\n",
    "    ig2 =  (ent - ep2 * r2)\n",
    "    print('Information gain', ent, ig1, ig2)\n",
    "    print()\n",
    "    if ig1 > ig2:\n",
    "        print('Reduce surprise by choosing split using key p1', ig1)\n",
    "    else:\n",
    "        print('Reduce surprise by choosing split using key p2', ig2)\n",
    "\n",
    "    print()\n",
    "    print('ii', d1, d2)\n",
    "\n",
    "    print('gain ratios [p1, p2]', ig1/d1, ig2/d2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nathanstruble/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/loss.py:529: UserWarning: Using a target size (torch.Size([4, 4])) that is different to the input size (torch.Size([2, 2, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (4) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/d6/9_0p0ctd5cd5q3f8lhpbzpc40000gn/T/ipykernel_61424/3102068276.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;31m#loss += (((check_map - (pred*4.0).round()).abs()/4.0).sum())/2.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0ml_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcheck_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# + (((check_map - (pred*4.0).round()).abs()/4.0).sum())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0medg_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3259\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3261\u001b[0;31m     \u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3262\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/functional.py\u001b[0m in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (4) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "# DISCRIMINATOR EXPERIMENTS\n",
    "\n",
    "check_map = torch.zeros((height, width))\n",
    "\n",
    "val, id = torch.max(map_wfc.flatten(-2,-1),dim=1)\n",
    "#print(id)\n",
    "id_map = id.reshape(10,10)\n",
    "\n",
    "\n",
    "class discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.c1 = nn.Conv2d(1,16, 5, 2, 2)\n",
    "        self.c2 = nn.Conv2d(1, 16, 5, 2, 2)\n",
    "\n",
    "        self.c3 = nn.Conv2d(16,16,3,1,1)\n",
    "        self.c4 = nn.Conv2d(16,16,3,1,1)\n",
    "\n",
    "        self.c5 = nn.Conv2d(32, 16, 3, 1, 1)\n",
    "\n",
    "        self.p1 = nn.Conv2d(16,1,1)\n",
    "        self.p2 = nn.Conv2d(16,1,1)\n",
    "\n",
    "        self.up = nn.UpsamplingBilinear2d(scale_factor=2)\n",
    "        self.cup = nn.ConvTranspose2d(16, 1, 3, 2)\n",
    "        self.pad = nn.ZeroPad2d(1)\n",
    "        self.readout = nn.Conv2d(49,1,3,1,1)\n",
    "\n",
    "        self.sigma = self.func #nn.Sigmoid()\n",
    "        self.phi = self.func #nn.Tanh()\n",
    "        self.rel = nn.ReLU() #nn.LeakyReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        self.cc1 = nn.Conv2d(1,1,3,1,0)\n",
    "        self.cc2 = nn.Conv2d(1,1,3,1,0)\n",
    "        self.cc3 = nn.Conv2d(1,1,3,1,0)\n",
    "        self.cc4 = nn.Conv2d(1,1,3,1,0)\n",
    "\n",
    "        self.cp = nn.Conv2d(4,2,1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if 0:\n",
    "            # Reduce\n",
    "            y = self.c1(x)\n",
    "            z = self.c2(x)\n",
    "            # Permute\n",
    "            zz = self.rel(self.c4(z))\n",
    "            yy = self.c3(self.phi(y))\n",
    "            pz = self.sigma(self.p1(zz))\n",
    "            # Gate\n",
    "            yz = torch.concat((yy, zz), dim=0)\n",
    "            yz = self.c5(yz)\n",
    "            xx = zz * pz + (1 - pz) * yz\n",
    "            # Expand\n",
    "            xc = self.up(yy.unsqueeze(0))\n",
    "            xx = self.p2(self.up(zz.unsqueeze(0)))\n",
    "            pred = torch.concat((xx, xc, self.up(y.unsqueeze(0)), self.up(z.unsqueeze(0))),dim=1)\n",
    "            pred = self.sigmoid(self.readout(pred))\n",
    "\n",
    "        x1 = self.cc1(x)\n",
    "        x2 = self.rel(self.cc2(x))\n",
    "        x3 = self.cc3(x)\n",
    "        x4 = self.rel(self.cc4(x))\n",
    "        xx = torch.concat((x1,x2,x3,x4), dim = 0)\n",
    "        pred = self.cp(xx)\n",
    "\n",
    "        return pred\n",
    "    \n",
    "    def func(self, x):\n",
    "        return x\n",
    "\n",
    "batch_size = 1\n",
    "epochs = 1\n",
    "loss = 0\n",
    "\n",
    "width = 4 \n",
    "height = 4\n",
    "\n",
    "l_func = nn.MSELoss()\n",
    "disc = discriminator()\n",
    "optimizer_disc = torch.optim.SGD(disc.parameters(), lr = 0.0001)\n",
    "\n",
    "edge_loss = torch.zeros((3,3))\n",
    "edge_loss[:,0] = -1\n",
    "edge_loss[:,2] = 1\n",
    "edge_loss[1,0] = -2\n",
    "edge_loss[1,2] = 2\n",
    "edge_loss = edge_loss.unsqueeze(0)\n",
    "\n",
    "block = torch.zeros((2,4,4))\n",
    "block[1,:,:] = 1\n",
    "\n",
    "pad = torch.nn.ZeroPad2d(1)\n",
    "\n",
    "for e in range(epochs):\n",
    "    id_map_rand = torch.randint(0, 2, (width,height))\n",
    "    id_check = id_map_rand\n",
    "    check_map = torch.zeros((height, width))\n",
    "\n",
    "    for i in range(width):\n",
    "        for j in range(height):\n",
    "            tile = id_check[j,i]\n",
    "            n1 = n2 = n3 = n4 = -1\n",
    "            if j > 0:\n",
    "                n1 = id_check[j-1,i]\n",
    "            if j < height-1:\n",
    "                n2 = id_check[j+1,i]\n",
    "            if i > 0:\n",
    "                n3 = id_check[j,i-1]\n",
    "            if i < width-1:\n",
    "                n4 = id_check[j,i+1]\n",
    "\n",
    "            vec = torch.from_numpy(np.array((n1,n2,n3,n4)))\n",
    "            \n",
    "            for k in range(4):\n",
    "                if vec[k] > -0.5:\n",
    "                    check_map[j,i] += key[tile, vec[k]]\n",
    "            \n",
    "    \n",
    "    pred = disc.forward(id_map_rand.unsqueeze(0).float()).squeeze()\n",
    "\n",
    "    #loss += (((check_map - (pred*4.0).round()).abs()/4.0).sum())/2.0\n",
    "    loss += l_func((pred), (check_map).detach().squeeze()) # + (((check_map - (pred*4.0).round()).abs()/4.0).sum())\n",
    "\n",
    "    edg_pred = nn.functional.conv2d((pred).unsqueeze(0).unsqueeze(0), edge_loss.unsqueeze(0), padding = 0)\n",
    "    edg_pred += nn.functional.conv2d((pred).unsqueeze(0).unsqueeze(0), edge_loss.transpose(-2,-1).unsqueeze(0), padding = 0)\n",
    "    edg_gt = nn.functional.conv2d((check_map/4.0).unsqueeze(0).unsqueeze(0), edge_loss.unsqueeze(0), padding = 0)\n",
    "    loss += ((18-edg_pred) / 18.0).mean()\n",
    "\n",
    "    if e%(500) == 0 and e > 0:        \n",
    "        print('Avg loss:', loss.detach().numpy()/batch_size)\n",
    "\n",
    "    if e%batch_size == 0 and e > 0:\n",
    "        optimizer_disc.zero_grad()\n",
    "        loss = loss / batch_size\n",
    "        loss.backward()\n",
    "        optimizer_disc.step()\n",
    "        del loss\n",
    "        loss = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(check_map.sum())\n",
    "print('Greater number is better')\n",
    "plt.figure(1)\n",
    "plt.imshow((check_map/4.0).numpy())\n",
    "plt.colorbar()\n",
    "\n",
    "plt.figure(2)\n",
    "plt.imshow((pred).squeeze().detach().numpy())\n",
    "plt.colorbar()\n",
    "\n",
    "plt.figure(3)\n",
    "plt.imshow((nn.functional.conv2d((pred).unsqueeze(0).unsqueeze(0), edge_loss.unsqueeze(0), padding = 0)).squeeze().detach().numpy())\n",
    "plt.colorbar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 814,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class autoencdoer(nn.Module):\n",
    "    def __init__(self, w, h):\n",
    "        super().__init__()\n",
    "\n",
    "        self.width = w\n",
    "        self.height = h\n",
    "        self.c1 = nn.Conv2d(1, 16, 3, 2, 1)\n",
    "        self.c2 = nn.Conv2d(16, 32, 3, 2, 1)\n",
    "        self.c3 = nn.Conv2d(32, 32, 3, 2, 1)\n",
    "        self.c4 = nn.Conv2d(32, 16, 1)\n",
    "        op = int((w/8 * h /8 * 16))\n",
    "        redux = w//2 * h //2\n",
    "        self.l1 = nn.Linear(op, redux)\n",
    "        self.l2 = nn.Linear(redux, redux)\n",
    "        self.l3 = nn.Linear(redux, op)\n",
    "        self.sigma = nn.ReLU()\n",
    "        self.pad = nn.ZeroPad2d(1)\n",
    "\n",
    "\n",
    "        self.u1 = nn.ConvTranspose2d(32, 32, 3, 2, 2)\n",
    "        self.u2 = nn.ConvTranspose2d(32, 32, 3, 2, 1)\n",
    "        self.u3 = nn.ConvTranspose2d(32, 32, 4, 2, 2)\n",
    "        self.readout = nn.Conv2d(32, 1, 1)\n",
    "        self.sigma = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encode(x)\n",
    "        x = self.latent(x)\n",
    "        x = self.decode(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.c1(x)\n",
    "        x = self.c2(x)\n",
    "        x = self.c3(x)\n",
    "        x = self.c4(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def latent(self, x):\n",
    "        y = x.reshape(x.shape[0], -1)\n",
    "        y = self.l1(y)\n",
    "        y = self.sigma(self.l2(y))\n",
    "        y = self.l3(y)\n",
    "        x = self.pad((y.reshape(x.shape) + x)/2.0)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def decode(self, x):\n",
    "        x = self.u1(x)\n",
    "        x = self.u2(x)\n",
    "        x = self.u3(x)\n",
    "        \n",
    "        x = self.sigma(self.readout(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class autoencoder16x16(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        r = 16*16\n",
    "\n",
    "        self.l1 = nn.Linear(r, r)\n",
    "        self.l2 = nn.Linear(r, r//2)\n",
    "        self.l3 = nn.Linear(r//2, r//2)\n",
    "        self.l4 = nn.Linear(r//2, r//4)\n",
    "        self.l5 = nn.Linear(r//4, r//4)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.rel = nn.LeakyReLU()\n",
    "\n",
    "        self.invl5 = nn.Linear(r//4, r//4)\n",
    "        self.invl4 = nn.Linear(r//4*2, r//2)\n",
    "        self.invl3 = nn.Linear(r//2*2, r//2)\n",
    "        self.invl2 = nn.Linear(r//2*2, r//2)\n",
    "        self.invl1 = nn.Linear(r//2, r)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(64*2)\n",
    "        self.bn2 = nn.BatchNorm1d(128*2)\n",
    "\n",
    "        self.drop = nn.Dropout(0.3)\n",
    "        \n",
    "\n",
    "    def forward(self, x, b):\n",
    "        x1, x2, x3, x4, x5 = self.encode(x)\n",
    "        y = self.decode(x2, x3, x4, x5, b)\n",
    "\n",
    "        return y\n",
    "\n",
    "    def encode(self, x):\n",
    "\n",
    "        x1 = self.l1(x)\n",
    "        x2 = self.l2(x1)\n",
    "        x3 = self.l3(x2)\n",
    "        x4 = self.l4(x3)\n",
    "        x5 = self.l5(x4)\n",
    "\n",
    "        return x1, x2, x3, x4, self.rel(x5)\n",
    "\n",
    "    def decode(self, x2, x3, x4, x5, b):\n",
    "        \n",
    "        v = torch.concat((self.invl5(x5), x4), dim=2).squeeze()\n",
    "     \n",
    "        y5 = self.bn1(v).reshape(b,-1).squeeze()\n",
    "        vv = torch.concat((self.invl4(y5), x3.squeeze()), dim=1)\n",
    "        y4 = self.drop(self.bn2(vv).reshape(b,-1))\n",
    "        v2 = self.invl3(y4)\n",
    "        y3 = torch.concat((v2, x2.squeeze()), dim=1).reshape(b,-1)\n",
    "        y2 = self.invl2(y3)\n",
    "        y1 = self.sigmoid(self.invl1(y2).squeeze())\n",
    "\n",
    "        return y1\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "numt_tiles = 13\n",
    "h = 16\n",
    "w = 16\n",
    "train_size = 50\n",
    "valid_size = 15\n",
    "test_size = 10\n",
    "\n",
    "if 0:\n",
    "\n",
    "    map_list = torch.zeros(train_size, numt_tiles, h, w)\n",
    "    id_map_list = torch.zeros(train_size, h, w)\n",
    "\n",
    "    map_list_v = torch.zeros(valid_size, numt_tiles, h, w)\n",
    "    id_map_list_v = torch.zeros(valid_size, h, w)\n",
    "\n",
    "    map_list_t = torch.zeros(test_size, numt_tiles, h, w)\n",
    "    id_map_list_t = torch.zeros(test_size, h, w)\n",
    "\n",
    "    key = gen_key(num_tiles, 6, 6)\n",
    "    m_l, i_l = gen_map(key, h, w, numt_tiles)\n",
    "\n",
    "    for k in range(train_size):\n",
    "        m_l, i_l = gen_map(key, h, w, numt_tiles)\n",
    "        map_list[k] = m_l\n",
    "        id_map_list[k] = i_l\n",
    "        \n",
    "\n",
    "\n",
    "    for k in range(valid_size):\n",
    "        m_l, i_l = gen_map(key, h, w, numt_tiles)\n",
    "        map_list_v[k] = m_l\n",
    "        id_map_list_v[k] = i_l\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    for k in range(test_size):\n",
    "        m_l, i_l = gen_map(key, h, w, numt_tiles)\n",
    "        map_list_t[k] = m_l\n",
    "        id_map_list_t[k] = i_l\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.FloatTensor\n",
      "17\n",
      "torch.Size([13, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(map_list.type())\n",
    "index = np.arange(50)\n",
    "np.random.shuffle(index)\n",
    "\n",
    "i = int(index[int(b + i2*batch_size)])\n",
    "\n",
    "\n",
    "print(i)\n",
    "\n",
    "m = map_id_list[i].clone()\n",
    "\n",
    "print(m.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 818,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 13, 16, 16])\n",
      "training loss epoch 100 = 0.503515613079071\n",
      "Validation loss for epoch 100 = 0.47304126620292664\n",
      "Testing error 0.47878599166870117\n",
      "training loss epoch 200 = 0.4965384781360626\n",
      "Validation loss for epoch 200 = 0.49515411257743835\n",
      "Testing error 0.4843924045562744\n",
      "training loss epoch 300 = 0.4979206740856171\n",
      "Validation loss for epoch 300 = 0.503585696220398\n",
      "Testing error 0.4946514070034027\n",
      "training loss epoch 400 = 0.49871997237205506\n",
      "Validation loss for epoch 400 = 0.5229532122612\n",
      "Testing error 0.516826868057251\n",
      "training loss epoch 500 = 0.4996274054050446\n",
      "Validation loss for epoch 500 = 0.5066304802894592\n",
      "Testing error 0.49765628576278687\n",
      "training loss epoch 600 = 0.49963942766189573\n",
      "Validation loss for epoch 600 = 0.506290078163147\n",
      "Testing error 0.49765628576278687\n",
      "training loss epoch 700 = 0.49963942766189573\n",
      "Validation loss for epoch 700 = 0.506290078163147\n",
      "Testing error 0.49765628576278687\n",
      "training loss epoch 800 = 0.499609375\n",
      "Validation loss for epoch 800 = 0.506290078163147\n",
      "Testing error 0.49765628576278687\n",
      "training loss epoch 900 = 0.499609375\n",
      "Validation loss for epoch 900 = 0.506290078163147\n",
      "Testing error 0.49765628576278687\n"
     ]
    }
   ],
   "source": [
    "\n",
    "hh = 16\n",
    "epochs = 1000\n",
    "batch_size = 10\n",
    "auto = autoencoder16x16()\n",
    "optimizer_auto = torch.optim.NAdam(auto.parameters(), lr = 0.005)\n",
    "loss = nn.L1Loss()\n",
    "\n",
    "index = np.arange(train_size)\n",
    "\n",
    "print(map_list.shape)\n",
    "\n",
    "for i1 in range(epochs):\n",
    "    np.random.shuffle(index)\n",
    "    l_sum = 0\n",
    "\n",
    "    for i2 in range(train_size // batch_size):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            map_batch = torch.zeros((batch_size, 16 * 16))\n",
    "            for b in range(batch_size):\n",
    "                m = id_map_list[index[b + i2*batch_size]].clone().squeeze().reshape(1,-1)  / num_tiles\n",
    "                map_batch[b] = m\n",
    "       \n",
    "        out = auto.forward(map_batch.unsqueeze(1), b+1)\n",
    "        out = out.round()\n",
    "        \n",
    "        l_ = loss(out.squeeze(), map_batch)\n",
    "\n",
    "        l_sum += l_.clone().detach().numpy()\n",
    "\n",
    "        optimizer_auto.zero_grad()\n",
    "        l_.backward()\n",
    "        optimizer_auto.step()\n",
    "\n",
    "        del l_\n",
    "    \n",
    "    \n",
    "    if i1 % 20 == 0 and i1 > 0:\n",
    "\n",
    "        if i1 % 100 == 0:\n",
    "            print('training loss epoch', i1, '=', l_sum/(5))\n",
    "\n",
    "        l_sum = 0\n",
    "\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            map_batch = torch.zeros((valid_size, 16 * 16))\n",
    "            for b in range(valid_size):\n",
    "                m = id_map_list_v[b].clone().squeeze().reshape(1,-1) / num_tiles\n",
    "                map_batch[b] = m\n",
    "            \n",
    "        out = auto.forward(map_batch.unsqueeze(1), b+1)\n",
    "        l_ = loss(out.squeeze(), map_batch)\n",
    "\n",
    "        l_sum += l_.clone().detach().numpy()\n",
    "\n",
    "        optimizer_auto.zero_grad()\n",
    "        l_.backward()\n",
    "        optimizer_auto.step()\n",
    "\n",
    "        del l_\n",
    "\n",
    "        if i1 % 100 == 0:\n",
    "            print('Validation loss for epoch',i1,'=', l_sum)\n",
    "\n",
    "        l_sum = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            map_batch = torch.zeros((test_size, 16 * 16))\n",
    "            for b in range(test_size):\n",
    "                m = id_map_list_v[b].clone().squeeze().reshape(1,-1) / num_tiles\n",
    "                map_batch[b] = m\n",
    "                \n",
    "            out = auto.forward(map_batch.unsqueeze(1), b+1)\n",
    "            l_ = loss(out.squeeze(), map_batch)\n",
    "            l_sum += l_.clone().detach().numpy()\n",
    "\n",
    "            del l_\n",
    "            \n",
    "            if i1 % 100 == 0:\n",
    "            \n",
    "                print('Testing error',l_sum)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Failed auto encoders :()\n",
    "\n",
    "class AE(torch.nn.Module):\n",
    "    def __init__(self, num_tiles, bat):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ntiles = num_tiles\n",
    "        self.bs = bat\n",
    "\n",
    "        self.key_1 = torch.nn.Linear(num_tiles, num_tiles//2)\n",
    "        self.key_2 = torch.nn.Linear(num_tiles, num_tiles//2)\n",
    "        self.key_11 = torch.nn.Linear(num_tiles//2, num_tiles//4)\n",
    "        self.key_22 = torch.nn.Linear(num_tiles//2,num_tiles//4)\n",
    "        self.key_conv_1 = torch.nn.Conv2d(1,1,3,2,1)\n",
    "        self.key_conv_2 = torch.nn.Conv2d(1,1,3,2,1)\n",
    "        self.key_conv_3 = torch.nn.Conv2d(1,1,3,2,1)\n",
    "\n",
    "        self.lin1 = torch.nn.Linear((num_tiles//8)**2, num_tiles)\n",
    "        self.lin2 = torch.nn.Linear(num_tiles, num_tiles)\n",
    "        self.lin3 = torch.nn.Linear(num_tiles, num_tiles)\n",
    "        self.relu = torch.nn.LeakyReLU()\n",
    "        self.sig = torch.nn.Sigmoid()\n",
    "\n",
    "        self.square_lin = torch.nn.Linear(num_tiles, (num_tiles+num_tiles%2))\n",
    "        self.up_conv1 = torch.nn.Conv2d(2,2,3,1)\n",
    "        self.up_conv2 = torch.nn.Conv2d(2,1,3,1)\n",
    "        self.up_conv3 = torch.nn.Conv2d(1,1,3,1)\n",
    "        self.up_conv4 = torch.nn.Conv2d(1,1,3,1)\n",
    "        self.up_conv5 = torch.nn.Conv2d(1,1,3,1,1)\n",
    "\n",
    "        self.upscale = torch.nn.UpsamplingBilinear2d(scale_factor = 2)\n",
    "\n",
    "    def encode(self, x):\n",
    "        \n",
    "        key = x\n",
    "        row_ = self.key_11(self.key_1(key))\n",
    "        i1 = self.key_2(key.transpose(-2,-1))\n",
    "        col_ = self.key_22(i1)\n",
    "        cross_ = torch.matmul(row_,col_.transpose(-2,-1))\n",
    "        redux = self.key_conv_3(self.key_conv_2((self.key_conv_1(cross_))))\n",
    "\n",
    "        return redux\n",
    "    \n",
    "    def latent(self, x):\n",
    "        \n",
    "        o1 = (self.lin1(x.reshape(self.bs, -1).squeeze()))\n",
    "        o2 = self.lin2(o1)\n",
    "        o3 = (self.lin3(o2))\n",
    "\n",
    "        return o3\n",
    "\n",
    "    def decode(self, x):\n",
    "\n",
    "        x = self.square_lin(x)\n",
    "        x = x.reshape(self.bs,2, int(np.sqrt(x.shape[1]//2)), -1)\n",
    "        u1 = self.up_conv1(x)\n",
    "        u1_up = self.upscale(u1)\n",
    "        u2 = self.up_conv2(u1_up)\n",
    "        u2_up = self.upscale(u2)\n",
    "        u3 = self.up_conv3(u2_up)\n",
    "        u3_up = self.upscale(u3)\n",
    "        u4 = self.up_conv4(u3_up)\n",
    "        u4_up = self.upscale(u4)\n",
    "        u5 = self.up_conv5(u4_up)\n",
    "        u5_up = self.sig(self.upscale(u5))\n",
    "\n",
    "        return u5_up\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        encoded = self.encode(x)\n",
    "        latent = self.latent(encoded)\n",
    "        decoded = self.decode(latent)\n",
    "\n",
    "        return decoded\n",
    "\n",
    "\n",
    "class AE2(torch.nn.Module):\n",
    "    def __init__(self, batch_size, num_tiles):\n",
    "        super().__init__()\n",
    "        self.bs = batch_size\n",
    "        self.key_1 = torch.nn.Linear(num_tiles, num_tiles//2)\n",
    "        self.key_2 = torch.nn.Linear(num_tiles, num_tiles//2)\n",
    "        self.key_11 = torch.nn.Linear(num_tiles//2, num_tiles//4)\n",
    "        self.key_22 = torch.nn.Linear(num_tiles//2,num_tiles//4)       \n",
    "        self.key_111 = torch.nn.Linear(num_tiles//4, num_tiles//4)\n",
    "        self.key_222 = torch.nn.Linear(num_tiles//4,num_tiles//4)    \n",
    "\n",
    "        self.lin1 = torch.nn.Linear((num_tiles//4)**2, ((num_tiles//4)**2))\n",
    "        self.lin2 = torch.nn.Linear(((num_tiles//4)**2), ((num_tiles//4)**2))\n",
    "        self.relu = torch.nn.LeakyReLU()\n",
    "\n",
    "        self.upconv = torch.nn.UpsamplingNearest2d(scale_factor=2)\n",
    "        self.mx = torch.nn.AdaptiveMaxPool2d((50,50))\n",
    "        self.conv1 = torch.nn.Conv2d(1,1,3,1,1)\n",
    "        self.conv2 = torch.nn.Conv2d(1,1,3,1,2)\n",
    "        self.conv3 = torch.nn.Conv2d(1,1,3,1,1)\n",
    "\n",
    "        self.sig = torch.nn.Sigmoid()\n",
    "        \n",
    "\n",
    "    def encode(self,x):\n",
    "        key = x\n",
    "        row_ = self.key_111(self.key_11(self.key_1(key)))\n",
    "        col_ = self.key_222(self.key_22(self.key_2(key.transpose(-2,-1))))\n",
    "        mul = torch.matmul(row_.transpose(-2,-1),col_)\n",
    "        return mul\n",
    "    \n",
    "    def latent(self, x):\n",
    "        \n",
    "        x = self.relu(self.lin1(x.reshape(self.bs,-1)))\n",
    "        x = self.lin2(x)\n",
    "        return x\n",
    "    \n",
    "    def decode(self, x):\n",
    "\n",
    "        x = x.reshape(self.bs, int(np.sqrt(x.shape[1])), -1).unsqueeze(1)\n",
    "        x = self.upconv(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.upconv(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.upconv(x)\n",
    "        x = self.mx(x)\n",
    "        x = self.conv3(x)\n",
    "\n",
    "        return self.sig(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.encode(key)\n",
    "        lat = self.latent(x)\n",
    "        d = self.decode(lat)\n",
    "\n",
    "        return d\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 978,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0052, grad_fn=<L1LossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class ae(nn.Module):\n",
    "    def __init__(self):\n",
    "        latent = 1\n",
    "        super().__init__()\n",
    "        self.f1 =  nn.Linear(3, latent, bias = False)\n",
    "        self.f2 = nn.Linear(latent, 3)\n",
    "        self.act = self.pc #nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.f2(self.act(self.f1(x)))\n",
    "    \n",
    "    def pc(self,x):\n",
    "        return x\n",
    "\n",
    "enc = ae()\n",
    "\n",
    "opt = torch.optim.SGD(enc.parameters(), lr = 0.01)\n",
    "l = nn.L1Loss()\n",
    "\n",
    "\n",
    "x = torch.zeros((10,3))\n",
    "\n",
    "loss = 0\n",
    "\n",
    "# 1000,000 iterations\n",
    "for i in range(10000):\n",
    "    with torch.no_grad():\n",
    "        x[:] = torch.randint(0,2,(10,1)).float()\n",
    "        x[:,1] = 1-x[:,0]\n",
    "        x[:,2] = x[:,0]\n",
    "\n",
    "    y = enc(x)\n",
    "    loss = l(y,x)\n",
    "\n",
    "    if 1:\n",
    "        opt.zero_grad()\n",
    "        (loss).backward()\n",
    "        opt.step()\n",
    "\n",
    "\n",
    "\n",
    "print(loss)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
